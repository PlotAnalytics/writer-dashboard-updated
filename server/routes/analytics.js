const express = require('express');
const jwt = require('jsonwebtoken');
const { BigQuery } = require('@google-cloud/bigquery');
const { Storage } = require('@google-cloud/storage');
const path = require('path');

const router = express.Router();

// Import PostgreSQL pool
const pool = require('../config/database');

// Simplified function to get account names from BigQuery (similar to getBigQueryAudienceRetention)
async function getAccountNameFromBigQuery(videoId, writerId) {
  try {
    console.log(`ğŸ“Š Getting account name for video ${videoId}, writer ${writerId}`);

    // Use the global BigQuery client
    const bigqueryClient = getBigQueryClient();
    if (!bigqueryClient) {
      throw new Error('BigQuery client not initialized');
    }

    // Extract YouTube video ID from PostgreSQL video ID
    const videoQuery = `SELECT url FROM video WHERE id = $1`;
    const { rows: videoRows } = await pool.query(videoQuery, [parseInt(videoId)]);

    if (videoRows.length === 0) {
      throw new Error(`Video with ID ${videoId} not found`);
    }

    const videoUrl = videoRows[0].url;
    const youtubeVideoId = extractVideoId(videoUrl);

    if (!youtubeVideoId) {
      throw new Error(`Could not extract YouTube video ID from URL: ${videoUrl}`);
    }

    console.log(`ğŸ” Extracted YouTube video ID: ${youtubeVideoId} from URL: ${videoUrl}`);

    // Get writer name from PostgreSQL
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [parseInt(writerId)]);

    if (writerRows.length === 0) {
      throw new Error(`Writer with ID ${writerId} not found`);
    }

    const writerName = writerRows[0].name;
    console.log(`ğŸ‘¤ Found writer name: ${writerName}`);

    const projectId = process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";
    const dataset = "dbt_youtube_analytics";

    // Query BigQuery for account name using the same approach as individual videos
    const accountQuery = `
      SELECT DISTINCT
        account_id,
        video_id
      FROM \`${projectId}.${dataset}.audience_retention_historical\`
      WHERE video_id = @video_id
        AND writer_id = @writer_id
      LIMIT 1
    `;

    const [accountRows] = await bigqueryClient.query({
      query: accountQuery,
      params: {
        video_id: youtubeVideoId,
        writer_id: parseInt(writerId)
      }
    });

    if (accountRows.length > 0) {
      const accountId = accountRows[0].account_id;
      console.log(`ğŸ“Š Found account_id: ${accountId} for video ${youtubeVideoId}`);

      // Get account name from PostgreSQL posting_accounts table
      const accountNameQuery = `SELECT account_name FROM posting_accounts WHERE id = $1`;
      const { rows: accountNameRows } = await pool.query(accountNameQuery, [accountId]);

      if (accountNameRows.length > 0) {
        const accountName = accountNameRows[0].account_name;
        console.log(`âœ… Found account name: ${accountName} for account_id: ${accountId}`);
        return accountName;
      }
    }

    console.log(`âŒ No account name found for video ${videoId}`);
    return null;

  } catch (error) {
    console.error('âŒ Error getting account name from BigQuery:', error);
    return null;
  }
}

// Initialize InfluxDB service
let influxService;
try {
  const InfluxService = require('../services/influxService');
  // Set credentials
  process.env.INFLUXDB_URL = 'https://us-east-1-1.aws.cloud2.influxdata.com';
  process.env.INFLUXDB_TOKEN = 'ojNizGw1U0VID3ltz1khIx2aOQAHG0gIFEbR7VqVk6Ns23fzXOcJG-JxPkGKWL6lluFBQKdagMRbHm6-2iVHSw==';
  process.env.INFLUXDB_ORG = 'engineering team';
  process.env.INFLUXDB_BUCKET = 'youtube_api';
  influxService = new InfluxService();
  console.log('âœ… InfluxDB service initialized for analytics');
} catch (error) {
  console.error('âŒ Failed to initialize InfluxDB for analytics:', error);
}

// Load environment variables from root directory
require('dotenv').config({ path: require('path').join(__dirname, '../../.env') });

// BigQuery setup with admin_dashboard.json credentials
const setupBigQueryClient = async () => {
  try {
    const fs = require('fs');
    const path = require('path');

    // Try to load credentials from admin_dashboard.json file first
    let credentials;
    const credentialsPath = path.join(__dirname, '..', '..', 'admin_dashboard.json');

    if (fs.existsSync(credentialsPath)) {
      console.log(`ğŸ” Analytics BigQuery: Loading credentials from admin_dashboard.json`);
      const credentialsFile = fs.readFileSync(credentialsPath, 'utf8');
      credentials = JSON.parse(credentialsFile);
    } else {
      // Fallback to environment variable
      const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON;

      if (!credentialsJson) {
        throw new Error('Neither admin_dashboard.json file nor GOOGLE_APPLICATION_CREDENTIALS_JSON environment variable found');
      }

      console.log(`ğŸ” Analytics BigQuery: Loading credentials from environment variable`);
      credentials = JSON.parse(credentialsJson);
    }

    const projectId = credentials.project_id || process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";

    const bigquery = new BigQuery({
      credentials: credentials,
      projectId: projectId,
      location: "US",
    });

    console.log(`âœ… Analytics BigQuery client initialized successfully for project: ${projectId}`);
    console.log(`âœ… Analytics using service account: ${credentials.client_email}`);
    return bigquery;
  } catch (error) {
    console.error("âŒ Failed to set up BigQuery client:", error);
    throw error;
  }
};

// Use global BigQuery client instead of local initialization
let bigquery = null;

// Initialize BigQuery client on startup
const initializeBigQuery = async () => {
  try {
    bigquery = await setupBigQueryClient();
    console.log('âœ… Analytics routes: BigQuery client initialized successfully');
  } catch (error) {
    console.error('âŒ Analytics routes: Failed to initialize BigQuery client:', error);
  }
};

// Initialize immediately
initializeBigQuery();

const getBigQueryClient = () => {
  // Use global client if available, otherwise try local initialization
  if (global.bigqueryClient) {
    return global.bigqueryClient;
  }

  // Fallback to local client if global not available
  return bigquery;
};

// BigQuery helper functions - Updated to use youtube_video_report_historical exactly as QA script
async function getBigQueryViews(writerId, startDate, endDate, influxService = null) {
  try {
    console.log(`ğŸ“Š QA Script Analytics: Getting views for writer ${writerId} from ${startDate} to ${endDate}`);

    // Use global BigQuery client
    const bigqueryClient = getBigQueryClient();
    if (!bigqueryClient) {
      throw new Error('BigQuery client not initialized');
    }

    // First, get the writer name from PostgreSQL for BigQuery filtering
    console.log(`ğŸ” Getting writer name for writer_id=${writerId}`);
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);

    if (writerRows.length === 0) {
      throw new Error(`No writer found with id=${writerId}`);
    }

    const writerName = writerRows[0].name;
    console.log(`âœ… Found writer: ${writerName}`);

    let allData = [];

    // 1. Get BigQuery daily totals from youtube_video_report_historical (EXACTLY as QA script)
    try {
      // CONVERT date_day to string in SQL to prevent timezone issues
      const dailyTotalsQuery = `
        SELECT
          FORMAT_DATE('%Y-%m-%d', date_day) as date_string,
          COUNT(DISTINCT video_id) AS unique_videos,
          SUM(CAST(views AS INT64)) AS total_views
        FROM \`speedy-web-461014-g3.dbt_youtube_analytics.youtube_video_report_historical\`
        WHERE writer_name = @writer_name
          AND date_day BETWEEN @start_date AND @end_date
          AND date_day <= DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY)
          AND writer_name IS NOT NULL
          AND views IS NOT NULL
        GROUP BY date_day
        ORDER BY date_day DESC
        LIMIT 30;
      `;

      console.log(`ğŸ” BigQuery DAILY TOTALS query (EXCLUDING LAST 3 DAYS IN QUERY):`, dailyTotalsQuery);
      console.log(`ğŸ” BigQuery params:`, {
        writer_name: writerName,
        start_date: startDate,
        end_date: endDate
      });

      const [bigQueryRows] = await bigqueryClient.query({
        query: dailyTotalsQuery,
        params: {
          writer_name: writerName,
          start_date: startDate,
          end_date: endDate
        }
      });

      console.log(`ğŸ“Š BigQuery returned ${bigQueryRows.length} daily totals from youtube_video_report_historical`);

      // Transform BigQuery daily totals data - using FORMAT_DATE string
      const bigQueryData = bigQueryRows.map(row => ({
        time: { value: row.date_string }, // FORMAT_DATE returns a string, no timezone issues!
        views: parseInt(row.total_views || 0),
        unique_videos: parseInt(row.unique_videos || 0),
        source: 'BigQuery_Daily_Totals'
      }));

      allData = [...allData, ...bigQueryData];
      console.log(`âœ… Added ${bigQueryData.length} BigQuery daily totals data points (EXACTLY as QA script)`);

      // Create a set of dates we have BigQuery data for
      const bigQueryDates = new Set(bigQueryData.map(item => item.time.value));
      console.log(`ğŸ“Š BigQuery covers dates:`, Array.from(bigQueryDates).slice(0, 5));
      console.log(`ğŸ“Š BigQuery raw data sample:`, bigQueryData.slice(0, 3).map(item => ({
        date: item.time.value,
        views: item.views,
        video_title: item.video_title
      })));

      // 2. Get InfluxDB data for missing dates using hourly aggregation
      console.log(`ğŸ“Š Checking for missing dates to fill with InfluxDB hourly aggregation...`);

      // Generate all dates in the range
      const allDatesInRange = [];
      const currentDate = new Date(startDate);
      const endDateObj = new Date(endDate);

      while (currentDate <= endDateObj) {
        allDatesInRange.push(currentDate.toISOString().split('T')[0]);
        currentDate.setDate(currentDate.getDate() + 1);
      }

      const missingDates = allDatesInRange.filter(date => !bigQueryDates.has(date));
      console.log(`ğŸ“Š Missing dates for InfluxDB fallback:`, missingDates.slice(0, 5));

      if (missingDates.length > 0 && influxService) {
        try {
          // Calculate time range for InfluxDB to cover missing dates
          const daysDiff = Math.ceil((endDateObj - new Date(startDate)) / (1000 * 60 * 60 * 24));
          const influxRange = `${daysDiff + 5}d`; // Add buffer for InfluxDB

          console.log(`ğŸ” InfluxDB range for missing dates: ${influxRange}`);

          const dailyAnalytics = await influxService.getDashboardAnalytics(influxRange, writerId);

          // Filter InfluxDB data to only missing dates and convert UTC to EST
          const influxData = dailyAnalytics
            .filter(day => {
              // Convert InfluxDB UTC time to EST for date comparison
              const utcDate = new Date(day.date);
              const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
              const dayDate = estDate.toISOString().split('T')[0];
              return missingDates.includes(dayDate);
            })
            .map(day => {
              // Convert InfluxDB UTC time to EST
              const utcDate = new Date(day.date);
              const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
              return {
                time: { value: estDate.toISOString().split('T')[0] },
                views: day.views, // This is already hourly aggregated from InfluxDB
                source: 'InfluxDB_Hourly_Aggregation_EST'
              };
            });

          allData = [...allData, ...influxData];
          console.log(`âœ… Added ${influxData.length} InfluxDB hourly aggregated data points (UTCâ†’EST) for missing dates`);
        } catch (influxError) {
          console.error('âŒ InfluxDB hourly aggregation error:', influxError);
        }
      }

    } catch (bigQueryError) {
      console.error('âŒ BigQuery youtube_video_report_historical error:', bigQueryError);

      // Full fallback to InfluxDB if BigQuery fails completely
      if (influxService) {
        console.log('ğŸ”„ Full fallback to InfluxDB hourly aggregation');
        try {
          const daysDiff = Math.ceil((new Date(endDate) - new Date(startDate)) / (1000 * 60 * 60 * 24));
          const influxRange = `${daysDiff + 5}d`;

          const dailyAnalytics = await influxService.getDashboardAnalytics(influxRange, writerId);

          const influxData = dailyAnalytics
            .filter(day => {
              // Convert InfluxDB UTC time to EST for date comparison
              const utcDate = new Date(day.date);
              const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
              const dayDate = estDate.toISOString().split('T')[0];
              return dayDate >= startDate && dayDate <= endDate;
            })
            .map(day => {
              // Convert InfluxDB UTC time to EST
              const utcDate = new Date(day.date);
              const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
              return {
                time: { value: estDate.toISOString().split('T')[0] },
                views: day.views,
                source: 'InfluxDB_Full_Fallback_EST'
              };
            });

          allData = [...allData, ...influxData];
          console.log(`âœ… Full InfluxDB fallback (UTCâ†’EST): ${influxData.length} data points`);
        } catch (influxError) {
          console.error('âŒ Full InfluxDB fallback error:', influxError);
          throw new Error('Both BigQuery and InfluxDB failed');
        }
      }
    }

    // Add recent days using InfluxDB realtime data if missing
    const today = new Date().toISOString().split('T')[0];
    const yesterday = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString().split('T')[0];

    const hasToday = allData.some(item => item.time.value === today);
    const hasYesterday = allData.some(item => item.time.value === yesterday);

    if (!hasToday || !hasYesterday) {
      console.log(`ğŸ“Š Adding missing recent days: today=${!hasToday}, yesterday=${!hasYesterday}`);

      try {
        // Get recent data from InfluxDB for the last 3 days
        const recentData = await influxService.getDashboardAnalytics('3d', writerId);

        recentData.forEach(day => {
          const utcDate = new Date(day.date);
          const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
          const dayDate = estDate.toISOString().split('T')[0];

          // Only add if this date is missing and is today or yesterday
          if ((dayDate === today || dayDate === yesterday) && !allData.some(item => item.time.value === dayDate)) {
            allData.push({
              time: { value: dayDate },
              views: day.views,
              source: 'InfluxDB_Recent_Days'
            });
            console.log(`âœ… Added missing ${dayDate}: ${day.views} views from InfluxDB`);
          }
        });
      } catch (recentError) {
        console.error('âŒ Could not add recent days from InfluxDB:', recentError.message);
      }
    }

    // Add InfluxDB daily data for missing dates only (dotted line visualization)
    console.log(`ğŸ“Š Adding InfluxDB daily data for missing dates (dotted line visualization)...`);

    // Create a set of dates we already have BigQuery data for
    const existingDates = new Set(allData.map(item => item.time.value.split('T')[0]));
    console.log(`ğŸ“Š Existing dates from BigQuery:`, Array.from(existingDates).slice(0, 5));

    // Generate all dates in the range to find missing ones
    const allDatesInRange = [];
    const currentDate = new Date(startDate);
    const endDateObj = new Date(endDate);

    while (currentDate <= endDateObj) {
      allDatesInRange.push(currentDate.toISOString().split('T')[0]);
      currentDate.setDate(currentDate.getDate() + 1);
    }

    const missingDates = allDatesInRange.filter(date => !existingDates.has(date));
    console.log(`ğŸ“Š Missing dates for InfluxDB daily data:`, missingDates.slice(0, 5));

    if (missingDates.length > 0) {
      try {
        const daysDiff = Math.ceil((new Date(endDate) - new Date(startDate)) / (1000 * 60 * 60 * 24));
        const influxDailyData = await getDailyInfluxData(writerId, daysDiff + 5);

        if (influxDailyData.length > 0) {
          // Filter InfluxDB daily data to only missing dates
          const filteredInfluxDaily = influxDailyData.filter(item => {
            return missingDates.includes(item.time);
          });

          // Add InfluxDB daily data for missing dates only
          const influxDailyFormatted = filteredInfluxDaily.map(item => ({
            time: { value: item.time },
            views: item.views,
            source: item.source
          }));

          allData = [...allData, ...influxDailyFormatted];
          console.log(`âœ… Added ${influxDailyFormatted.length} InfluxDB daily data points for missing dates only`);
          console.log(`ğŸ“Š InfluxDB daily data sample:`, influxDailyFormatted.slice(0, 3));
        } else {
          console.log(`âš ï¸ No InfluxDB daily data available for missing dates`);
        }
      } catch (influxDailyError) {
        console.error('âŒ InfluxDB daily data error:', influxDailyError.message);
      }
    } else {
      console.log(`âœ… No missing dates - BigQuery covers all dates in range`);
    }

    // Sort all data by date
    allData.sort((a, b) => new Date(a.time.value) - new Date(b.time.value));

    console.log(`ğŸ“Š QA Script Strategy complete: ${allData.length} total data points`);
    console.log(`ğŸ“Š Date range coverage: ${allData[0]?.time.value} to ${allData[allData.length - 1]?.time.value}`);
    console.log(`ğŸ“Š Data sources used:`, [...new Set(allData.map(item => item.source))]);

    return allData;

  } catch (error) {
    console.error('âŒ getBigQueryViews QA Script error:', error);
    throw error;
  }
}

async function getBigQueryTopVideos(writerId, startDate, endDate, limit = 10) {
  try {
    console.log(`ğŸ¬ BigQuery: Getting top videos for writer ${writerId}`);

    // Get writer name from PostgreSQL
    const writerQuery = `
      SELECT name FROM writer WHERE id = $1
    `;
    const { rows: writerRows } = await pool.query(writerQuery, [parseInt(writerId)]);

    if (writerRows.length === 0) {
      throw new Error(`Writer with ID ${writerId} not found`);
    }

    const writerName = writerRows[0].name;
    console.log(`ğŸ“ Found writer name: ${writerName} for top videos`);

    const projectId = process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";
    const dataset = process.env.BIGQUERY_DATASET || "dashboard_prod";
    const table = process.env.BIGQUERY_TABLE || "daily_view_growth";

    // Simple PostgreSQL query to get top videos
    const query = `
      SELECT
        video.id as video_id,
        video.script_title AS title,
        video.url,
        COALESCE(statistics_youtube_api.views_total, 0) AS total_views,
        COALESCE(statistics_youtube_api.likes_total, 0) AS total_likes,
        COALESCE(statistics_youtube_api.comments_total, 0) AS total_comments,
        statistics_youtube_api.preview,
        statistics_youtube_api.duration,
        statistics_youtube_api.posted_date as first_date
      FROM video
      LEFT JOIN statistics_youtube_api
          ON CAST(video.id AS VARCHAR) = statistics_youtube_api.video_id
      WHERE video.writer_id = $1
        AND video.url LIKE '%youtube.com%'
        AND statistics_youtube_api.views_total IS NOT NULL
        AND statistics_youtube_api.posted_date BETWEEN $2 AND $3
      ORDER BY statistics_youtube_api.views_total DESC
      LIMIT $4
    `;

    const queryParams = [
      parseInt(writerId),
      startDate.toISOString(),
      endDate.toISOString(),
      parseInt(limit)
    ];

    console.log('ğŸ” PostgreSQL top videos query:', query);
    console.log('ğŸ“Š Query params:', queryParams);

    const { rows } = await pool.query(query, queryParams);
    console.log(`ğŸ¬ PostgreSQL returned ${rows.length} top videos for writer ${writerId}`);

    // Get video titles from PostgreSQL for better display
    const videoTitles = new Map();
    if (rows.length > 0) {
      try {
        const urls = rows.map(row => row.url).filter(url => url);
        if (urls.length > 0) {
          const titleQuery = `
            SELECT url, script_title
            FROM video
            WHERE url = ANY($1) AND writer_id = $2
          `;
          const { rows: titleRows } = await pool.query(titleQuery, [urls, parseInt(writerId)]);
          titleRows.forEach(row => {
            if (row.script_title) {
              videoTitles.set(row.url, row.script_title);
            }
          });
        }
      } catch (titleError) {
        console.error('âš ï¸ Error getting video titles:', titleError);
      }
    }

    return rows
      .filter(row => row.duration) // Only process videos with actual duration data
      .map(row => {
      // Use actual duration from database (no fallbacks)
      const duration = row.duration;

      // Determine video type based on duration (< 3 minutes = short, >= 3 minutes = video)
      let videoType = 'video'; // default
      let isShort = false;

      const parts = duration.split(':');
      if (parts.length >= 2) {
        const minutes = parseInt(parts[0]) || 0;
        const seconds = parseInt(parts[1]) || 0;
        const totalSeconds = minutes * 60 + seconds;

        if (totalSeconds < 180) { // Less than 3 minutes (180 seconds)
          videoType = 'short';
          isShort = true;
        }
      }

      return {
        id: row.video_id,
        title: row.title || `Video ${row.video_id}`,
        views: parseInt(row.total_views || 0),
        url: row.url,
        thumbnail: row.preview || (row.url ? `https://img.youtube.com/vi/${extractVideoId(row.url)}/maxresdefault.jpg` : '/path/to/default-thumbnail.jpg'),
        posted_date: row.first_date || new Date().toISOString(),
        type: videoType,
        isShort: isShort,
        duration: duration,
        engagement: Math.floor(Math.random() * 15) + 85, // 85-100% engagement
        likes: parseInt(row.total_likes || 0),
        comments: parseInt(row.total_comments || 0)
      };
    });
  } catch (error) {
    console.error('âŒ BigQuery top videos query error:', error);
    throw error;
  }
}

// PostgreSQL-based Content function with BigQuery account name enhancement
async function getPostgresContentVideosWithBigQueryNames(writerId, dateRange, page = 1, limit = 20, type = 'all') {
  try {
    console.log(`ğŸ¬ PostgreSQL Content: Getting videos for writer ${writerId}, range: ${dateRange}, type: ${type}`);

    // Get writer name from PostgreSQL
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [parseInt(writerId)]);

    if (writerRows.length === 0) {
      throw new Error(`Writer with ID ${writerId} not found`);
    }

    const writerName = writerRows[0].name;
    console.log(`ğŸ“ Found writer name: ${writerName} for content videos`);

    // Calculate date filter based on dateRange parameter
    let dateCondition = '';
    let queryParams = [parseInt(writerId)];

    if (dateRange !== 'lifetime') {
      const rangeNum = parseInt(dateRange) || 28;
      const dateFilter = new Date();
      dateFilter.setDate(dateFilter.getDate() - rangeNum);
      const dateFilterStr = dateFilter.toISOString().split('T')[0]; // YYYY-MM-DD format
      dateCondition = 'AND (s.posted_date >= $2 OR s.posted_date IS NULL)';
      queryParams.push(dateFilterStr);
      console.log(`ğŸ“… Date filter: Last ${rangeNum} days (since ${dateFilterStr})`);
    } else {
      console.log(`ğŸ“… Date filter: Lifetime (no date restriction)`);
    }

    // Step 1: Get videos from statistics_youtube_api for the writer with date filtering
    const postgresQuery = `
      SELECT
        v.id,
        v.script_title as title,
        v.url,
        v.writer_id,
        v.video_cat,
        COALESCE(s.views_total, 0) as views,
        COALESCE(s.likes_total, 0) as likes,
        COALESCE(s.comments_total, 0) as comments,
        s.posted_date,
        s.duration,
        s.preview,
        pa.account as account_name
      FROM video v
      INNER JOIN statistics_youtube_api s ON CAST(v.id AS VARCHAR) = s.video_id
      LEFT JOIN posting_accounts pa ON v.account_id = pa.id
      WHERE v.writer_id = $1
        AND (v.url LIKE '%youtube.com%' OR v.url LIKE '%youtu.be%')
        AND s.video_id IS NOT NULL
        ${dateCondition}
      ORDER BY s.posted_date DESC NULLS LAST, v.id DESC
    `;

    const { rows: postgresRows } = await pool.query(postgresQuery, queryParams);
    console.log(`ğŸ“Š PostgreSQL returned ${postgresRows.length} videos for writer ${writerId}`);

    // Step 2: Get duration data from BigQuery for accurate video type determination
    let bigQueryDurationMap = new Map();

    if (bigquery && postgresRows.length > 0) {
      try {
        const projectId = process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";
        const dataset = "dbt_youtube_analytics";

        // Query video report table for duration data (has video_duration_seconds)
        const reportQuery = `
          SELECT DISTINCT
            video_id,
            account_name,
            channel_title,
            video_duration_seconds
          FROM \`${projectId}.${dataset}.youtube_video_report_historical\`
          WHERE writer_name = @writer_name
            AND video_id IS NOT NULL
            AND video_duration_seconds IS NOT NULL
        `;

        console.log(`ğŸ” Querying BigQuery youtube_video_report_historical for duration data`);
        const [reportRows] = await bigquery.query({
          query: reportQuery,
          params: { writer_name: writerName }
        });

        console.log(`ğŸ“Š BigQuery youtube_video_report_historical returned ${reportRows.length} videos with duration data`);

        const bigQueryRows = reportRows;

        // Create map of video_id to duration data
        bigQueryRows.forEach(row => {
          if (row.video_id) {
            bigQueryDurationMap.set(row.video_id, {
              account_name: row.account_name,
              channel_title: row.channel_title,
              video_duration_seconds: row.video_duration_seconds
            });
          }
        });

        console.log(`ğŸ“Š BigQuery duration data available for ${bigQueryDurationMap.size} videos`);

        // Debug: Show sample BigQuery duration data
        if (bigQueryDurationMap.size > 0) {
          const sampleEntry = Array.from(bigQueryDurationMap.entries())[0];
          console.log(`ğŸ” Sample BigQuery duration data:`, {
            video_id: sampleEntry[0],
            account_name: sampleEntry[1].account_name,
            channel_title: sampleEntry[1].channel_title,
            duration_seconds: sampleEntry[1].video_duration_seconds
          });
        }
      } catch (bigQueryError) {
        console.log(`âš ï¸ BigQuery duration lookup failed, using PostgreSQL duration only:`, bigQueryError.message);
      }
    }

    // Step 3: Transform ALL PostgreSQL videos with BigQuery duration enhancement
    const videos = postgresRows.map(row => {
      // Extract YouTube video ID for BigQuery duration lookup
      const youtubeVideoId = extractVideoId(row.url);
      const bigQueryData = bigQueryDurationMap.get(youtubeVideoId) || {};

      // Debug: Show video ID matching for first few videos
      if (bigQueryDurationMap.size > 0 && Math.random() < 0.1) { // 10% chance to log
        console.log(`ğŸ” Video ID matching debug:`, {
          postgres_url: row.url,
          extracted_video_id: youtubeVideoId,
          bigquery_has_data: !!bigQueryData.account_name || !!bigQueryData.channel_title || !!bigQueryData.video_duration_seconds,
          bigquery_account_name: bigQueryData.account_name,
          bigquery_channel_title: bigQueryData.channel_title,
          bigquery_duration_seconds: bigQueryData.video_duration_seconds
        });
      }

      // Determine video type: BigQuery duration first, then PostgreSQL duration, then URL pattern
      let videoType = 'video'; // default to video (not short)
      let isShort = false;
      let durationSource = 'default';

      // Priority 1: Use BigQuery video_duration_seconds (most accurate)
      if (bigQueryData && bigQueryData.video_duration_seconds !== undefined && bigQueryData.video_duration_seconds !== null) {
        const durationSeconds = parseFloat(bigQueryData.video_duration_seconds);
        if (durationSeconds > 0 && durationSeconds < 183) { // Less than 3 minutes 3 seconds (183 seconds)
          videoType = 'short';
          isShort = true;
        } else if (durationSeconds >= 183) {
          videoType = 'video';
          isShort = false;
        }
        durationSource = 'bigquery';
        console.log(`ğŸ¬ Video type determined by BigQuery duration: ${durationSeconds}s = ${videoType}`);
      }
      // Priority 2: Use PostgreSQL duration as fallback (format: "HH:MM:SS" or "MM:SS")
      else if (row.duration && row.duration !== '0:00' && row.duration !== '00:00:00') {
        try {
          const parts = row.duration.split(':');
          let totalSeconds = 0;

          if (parts.length === 3) {
            // Format: "HH:MM:SS"
            const hours = parseInt(parts[0]) || 0;
            const minutes = parseInt(parts[1]) || 0;
            const seconds = parseInt(parts[2]) || 0;
            totalSeconds = (hours * 3600) + (minutes * 60) + seconds;
          } else if (parts.length === 2) {
            // Format: "MM:SS"
            const minutes = parseInt(parts[0]) || 0;
            const seconds = parseInt(parts[1]) || 0;
            totalSeconds = (minutes * 60) + seconds;
          }

          if (totalSeconds > 0 && totalSeconds < 183) { // Less than 3 minutes 3 seconds
            videoType = 'short';
            isShort = true;
          } else if (totalSeconds >= 183) {
            videoType = 'video';
            isShort = false;
          }
          durationSource = 'postgres';
          console.log(`ğŸ¬ Video type determined by PostgreSQL duration: ${totalSeconds}s (${row.duration}) = ${videoType}`);
        } catch (durationError) {
          console.log(`âš ï¸ Error parsing PostgreSQL duration "${row.duration}" for video ${youtubeVideoId}`);
          durationSource = 'error';
        }
      }

      // Priority 3: Use URL pattern as last resort
      if (durationSource === 'default' || durationSource === 'error') {
        if (row.url && row.url.includes('/shorts/')) {
          videoType = 'short';
          isShort = true;
          durationSource = 'url_pattern';
        } else {
          videoType = 'video';
          isShort = false;
          durationSource = 'url_pattern';
        }
        console.log(`ğŸ¬ Video type determined by URL pattern: ${videoType} (${row.url?.includes('/shorts/') ? 'shorts URL' : 'regular URL'})`);
      }

      // Calculate engagement rate
      const engagement = row.views > 0
        ? Math.round(((row.likes + row.comments) / row.views) * 100 * 100) / 100
        : 0;

      // Use whatever account name is available: BigQuery account_name, channel_title, or PostgreSQL account_name
      const enhancedAccountName = bigQueryData.account_name || bigQueryData.channel_title || row.account_name || 'Not Available';

      return {
        id: row.id,
        title: row.title || 'Untitled Video',
        url: row.url,
        writer_id: parseInt(writerId),
        writer_name: writerName,
        account_name: enhancedAccountName,
        preview: row.preview || `https://img.youtube.com/vi/${youtubeVideoId}/maxresdefault.jpg`,
        views: parseInt(row.views) || 0,
        likes: parseInt(row.likes) || 0,
        comments: parseInt(row.comments) || 0,
        posted_date: row.posted_date || new Date().toISOString(),
        duration: row.duration || '0:00',
        type: videoType,
        isShort: isShort,
        engagement: engagement,
        status: "Published",
        durationSource: durationSource, // Track where duration came from for debugging
        source: bigQueryData.account_name || bigQueryData.channel_title ? 'postgres_with_bigquery_duration' : 'postgres_only'
      };
    }); // Show ALL videos from PostgreSQL statistics_youtube_api table

    console.log(`ğŸ¬ Total videos from statistics_youtube_api: ${videos.length} videos for writer ${writerId}`);

    // Debug: Log sample video types before filtering
    console.log(`ğŸ” Sample video types before filtering:`, videos.slice(0, 5).map(v => ({
      id: v.id,
      title: v.title?.substring(0, 30),
      type: v.type,
      isShort: v.isShort,
      duration: v.duration,
      durationSource: v.durationSource,
      url: v.url?.includes('/shorts/') ? 'shorts_url' : 'regular_url'
    })));

    // Step 4: Apply type filtering
    let filteredVideos = videos;

    console.log(`ğŸ” Filtering by type: '${type}'`);

    if (type === 'short' || type === 'shorts') {
      filteredVideos = videos.filter(video => video.isShort);
      console.log(`ğŸ” Filtering for shorts: ${filteredVideos.length} videos where isShort=true`);
    } else if (type === 'video' || type === 'content') {
      filteredVideos = videos.filter(video => !video.isShort);
      console.log(`ğŸ” Filtering for videos: ${filteredVideos.length} videos where isShort=false`);
    }
    // If type === 'all', show all videos (no filtering)

    // Calculate type counts
    const shortCount = videos.filter(v => v.isShort).length;
    const videoCount = videos.filter(v => !v.isShort).length;

    console.log(`ğŸ¬ Type filtering results: ${filteredVideos.length}/${videos.length} videos match type '${type}'`);
    console.log(`ğŸ“Š Type breakdown: ${shortCount} shorts, ${videoCount} videos`);

    // Debug: Log sample filtered videos
    if (filteredVideos.length > 0) {
      console.log(`ğŸ” Sample filtered videos:`, filteredVideos.slice(0, 3).map(v => ({
        id: v.id,
        title: v.title?.substring(0, 30),
        type: v.type,
        isShort: v.isShort,
        duration: v.duration
      })));
    } else {
      console.log(`âš ï¸ No videos match the filter type '${type}'!`);
    }

    // Step 5: Apply pagination
    const startIndex = (parseInt(page) - 1) * parseInt(limit);
    const endIndex = startIndex + parseInt(limit);
    const paginatedVideos = filteredVideos.slice(startIndex, endIndex);

    console.log(`ğŸ“„ Pagination: Page ${page}, showing ${startIndex + 1}-${Math.min(endIndex, filteredVideos.length)} of ${filteredVideos.length} filtered videos`);

    return {
      videos: paginatedVideos,
      pagination: {
        currentPage: parseInt(page),
        totalPages: Math.ceil(filteredVideos.length / parseInt(limit)),
        totalVideos: filteredVideos.length,
        videosPerPage: parseInt(limit),
        hasNextPage: endIndex < filteredVideos.length,
        hasPrevPage: parseInt(page) > 1
      },
      typeCounts: {
        all: videos.length,
        short: shortCount,
        video: videoCount
      }
    };

  } catch (error) {
    console.error('âŒ PostgreSQL content videos query error:', error);
    throw error;
  }
}

// Helper function to extract video ID from YouTube URL
function extractVideoId(url) {
  if (!url) return 'dQw4w9WgXcQ'; // Default video ID

  // Handle YouTube Shorts URLs: https://youtube.com/shorts/VIDEO_ID
  const shortsMatch = url.match(/youtube\.com\/shorts\/([a-zA-Z0-9_-]{11})/);
  if (shortsMatch) {
    return shortsMatch[1];
  }

  // Handle regular YouTube URLs: https://youtube.com/watch?v=VIDEO_ID
  const regularMatch = url.match(/(?:youtube\.com\/(?:[^\/]+\/.+\/|(?:v|e(?:mbed)?)\/|.*[?&]v=)|youtu\.be\/)([^"&?\/\s]{11})/);
  return regularMatch ? regularMatch[1] : 'dQw4w9WgXcQ';
}

// Helper function to format duration from seconds to MM:SS format
function formatDuration(seconds) {
  if (!seconds || seconds === 0) return '0:00';

  const totalSeconds = parseInt(seconds);
  const minutes = Math.floor(totalSeconds / 60);
  const remainingSeconds = totalSeconds % 60;

  return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;
}

// Middleware to verify JWT token
const authenticateToken = (req, res, next) => {
  const token = req.headers.authorization?.split(' ')[1];

  if (!token) {
    return res.status(401).json({ message: 'No token provided' });
  }

  try {
    const decoded = jwt.verify(token, process.env.JWT_SECRET || 'fallback_secret');
    req.user = decoded;
    next();
  } catch (error) {
    res.status(401).json({ message: 'Invalid token' });
  }
};

// Assumes `bigquery` (BigQuery client) and `pool` (pg Pool) are initialized at module scope.
// Requires an InfluxDB service with a `queryFlux(flux: string): Promise<any[]>` method.

async function getBigQueryAnalyticsOverview(
  writerId,
  range = '30d',
  writerName = null,
  limit = 100,
  customStartDate = null,
  customEndDate = null
) {
  try {
    console.log(`ğŸ“Š ANALYTICS OVERVIEW: writer=${writerId} (${writerName}), range=${range}, limit=${limit}`);
    console.log(`ğŸ“Š Custom dates: start=${customStartDate}, end=${customEndDate}`);

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 1) Ensure BigQuery client â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    if (!bigquery) throw new Error('BigQuery client not initialized');

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 2) Lookup writerName if missing â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    if (!writerName) {
      const res = await pool.query('SELECT name FROM writer WHERE id = $1', [parseInt(writerId, 10)]);
      if (res.rows.length === 0) throw new Error(`Writer with ID ${writerId} not found`);
      writerName = res.rows[0].name;
    }

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 3) Compute date window â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    let finalStartDate, finalEndDate;

    if (customStartDate && customEndDate) {
      // Use custom date range
      finalStartDate = customStartDate;
      finalEndDate = customEndDate;
      console.log(`ğŸ“… Using CUSTOM date range: ${finalStartDate} â†’ ${finalEndDate}`);
    } else {
      // Use predefined range
      const endDate = new Date();
      const startDate = new Date();
      let days;
      switch (range) {
        case '7d':       days = 7;   break;
        case 'lifetime': days = 365; break;
        default:         days = 30;  break;
      }
      startDate.setDate(endDate.getDate() - days);
      finalStartDate = startDate.toISOString().slice(0, 10);
      finalEndDate = endDate.toISOString().slice(0, 10);
      console.log(`ğŸ“… Using PREDEFINED date range: ${finalStartDate} â†’ ${finalEndDate}`);
    }

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 4) Determine data source strategy â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    // REVERT TO ORIGINAL WORKING LOGIC: Use EDT (Eastern Daylight Time) which is UTC-4 during summer
    // Calculate current EDT time by subtracting 4 hours from UTC
    const nowUTC = new Date();
    const nowEDT = new Date(nowUTC.getTime() - (4 * 60 * 60 * 1000)); // Subtract 4 hours for EDT

    const threeDaysAgo = new Date(nowEDT);
    threeDaysAgo.setDate(nowEDT.getDate() - 3);
    const cutoffDateStr = threeDaysAgo.toISOString().slice(0, 10);

    console.log(`ğŸ“… REVERTED to original working logic:
      Current UTC: ${nowUTC.toISOString()}
      Current EDT: ${nowEDT.toISOString()}
      Cutoff date (3 days ago): ${cutoffDateStr}`);

    const requestStartDate = new Date(finalStartDate);
    const requestEndDate = new Date(finalEndDate);

    let useBigQuery = false;
    let useInfluxDB = false;
    let finalBigQueryEndDate = finalEndDate;
    let finalInfluxStartDate = cutoffDateStr;

    const bigQueryCutoffDate = new Date(cutoffDateStr);
    const influxCutoffDate = new Date(cutoffDateStr);

    if (requestEndDate <= bigQueryCutoffDate) {
      // Entire range is in BigQuery territory
      useBigQuery = true;
      useInfluxDB = false;
      finalBigQueryEndDate = finalEndDate;
      console.log(`ğŸ“Š Data source strategy: BigQuery ONLY (range ends ${finalEndDate}, BigQuery covers until ${cutoffDateStr})`);
    } else if (requestStartDate >= influxCutoffDate) {
      // Entire range is in InfluxDB territory
      useBigQuery = false;
      useInfluxDB = true;
      console.log(`ğŸ“Š Data source strategy: InfluxDB ONLY (range starts ${finalStartDate}, InfluxDB covers from ${cutoffDateStr})`);
    } else {
      // Range spans both territories
      useBigQuery = true;
      useInfluxDB = true;
      finalBigQueryEndDate = cutoffDateStr;
      finalInfluxStartDate = cutoffDateStr;
      console.log(`ğŸ“Š Data source strategy: BOTH (BigQuery until ${cutoffDateStr}, InfluxDB from ${cutoffDateStr})`);
    }

    console.log(`ğŸ” Date range analysis: {
      finalStartDate: '${finalStartDate}',
      finalEndDate: '${finalEndDate}',
      cutoffDateStr: '${cutoffDateStr}',
      useBigQuery: ${useBigQuery},
      useInfluxDB: ${useInfluxDB},
      finalBigQueryEndDate: '${finalBigQueryEndDate}',
      finalInfluxStartDate: '${finalInfluxStartDate}'
    }`);

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 5) QA: Raw Views from BigQuery â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

    let rawViewsRows = [];
    if (useBigQuery) {
      const rawViewsQuery = `
        SELECT
          FORMAT_DATE('%Y-%m-%d', date_day) as date_string,
          video_id,
          video_title,
          views,
          account_name
        FROM \`speedy-web-461014-g3.dbt_youtube_analytics.youtube_video_report_historical\`
        WHERE writer_name = @writer_name
          AND date_day >= @start_date
          AND date_day <= @end_date
          ${useBigQuery && useInfluxDB ? 'AND date_day <= DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY)' : ''}
          AND writer_name IS NOT NULL
          AND views IS NOT NULL
        ORDER BY date_day DESC, views DESC;

      `;
      const [bigQueryRawRows] = await bigquery.query({
        query: rawViewsQuery,
        params: {
          writer_name: writerName,
          start_date: finalStartDate,
          end_date: finalBigQueryEndDate,
        }
      });
      rawViewsRows = bigQueryRawRows;
    }
    console.log(`ğŸ“‹ Raw Views (${rawViewsRows.length} rows):`);
    console.table(rawViewsRows.slice(0, 100).map(row => ({
      date_string: row.date_string,
      video_id: row.video_id,
      video_title: row.video_title,
      views: row.views,
      account_name: row.account_name
    })));

    console.log('âœ… Raw views processing completed successfully');
    console.log('ğŸ” About to start daily totals query...');

    // â€”â€”â€” Daily totals via BigQuery (exclude last 3 days, use date picker range) â€”â€”â€”
    console.log('ğŸ” Executing BigQuery daily totals query...');

    let dailyTotalsRows = [];
    if (useBigQuery) {
      try {
        // CONVERT date_day to string in SQL to prevent timezone issues
        const dailyTotalsQuery = `
          SELECT
            FORMAT_DATE('%Y-%m-%d', date_day) as date_string,
            COUNT(DISTINCT video_id) AS unique_videos,
            SUM(CAST(views AS INT64)) AS total_views
          FROM \`speedy-web-461014-g3.dbt_youtube_analytics.youtube_video_report_historical\`
          WHERE writer_name = @writer_name
            AND date_day BETWEEN @start_date AND @end_date
            ${useBigQuery && useInfluxDB ? `AND date_day <= DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY)` : ''}
            AND writer_name IS NOT NULL
            AND views IS NOT NULL
          GROUP BY date_day
          ORDER BY date_day DESC
          LIMIT 30;
        `;

        console.log(`ğŸ” BigQuery DAILY TOTALS query (smart filtering):`, dailyTotalsQuery);
        console.log(`ğŸ” BigQuery params:`, {
          writer_name: writerName,
          start_date: finalStartDate,
          end_date: finalBigQueryEndDate
        });

        const [bigQueryRows] = await bigquery.query({
          query: dailyTotalsQuery,
          params: {
            writer_name: writerName,
            start_date: finalStartDate,
            end_date: finalBigQueryEndDate
          }
        });

        dailyTotalsRows = bigQueryRows;
        console.log(`ğŸ“Š BigQuery returned ${bigQueryRows.length} daily totals from youtube_video_report_historical`);

      } catch (bigQueryError) {
        console.error('âŒ BigQuery daily totals error:', bigQueryError);
        // Continue with empty array if BigQuery fails
        dailyTotalsRows = [];
      }
    }

    console.log('âœ… Daily totals query completed successfully');
    console.log(`ğŸ“‹ Daily Totals (${dailyTotalsRows.length} days):`);
    console.table(dailyTotalsRows);

    // â€”â€”â€” Summary stats from DAILY TOTALS (EXACTLY as QA script) â€”â€”â€”
    const totalViews   = dailyTotalsRows.reduce((sum, r) => sum + parseInt(r.total_views, 10), 0);
    const uniqueVideos = dailyTotalsRows.reduce((sum, r) => sum + parseInt(r.unique_videos, 10), 0);
    const uniqueDates  = dailyTotalsRows.length;
    console.log('ğŸ“Š Summary from DAILY TOTALS (EXACTLY as QA script):');
    console.log(`   Daily Total Rows: ${dailyTotalsRows.length}`);
    console.log(`   Total Views:      ${totalViews.toLocaleString()}`);
    console.log(`   Unique Videos:    ${uniqueVideos}`);
    console.log(`   Unique Dates:     ${uniqueDates}`);

    console.log('ğŸ¯ DAILY TOTALS PROCESSING COMPLETED SUCCESSFULLY!');
    console.log('ğŸš€ REACHED INFLUXDB SECTION - About to start InfluxDB integration...');

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 6) Get InfluxDB data (real-time data) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    const influxData = [];
    if (useInfluxDB) {
      try {
        console.log('ğŸ“Š Fetching InfluxDB data for real-time data...');
        console.log('ğŸ“Š About to initialize InfluxDB service...');
        const InfluxService = require('../services/influxService');
        const influx = new InfluxService();
        console.log('ğŸ“Š InfluxDB service initialized successfully');

        // Determine the range for InfluxDB based on the request
        let influxRange = '3d'; // default
        if (!useBigQuery) {
          // If only using InfluxDB, calculate the range from the custom dates
          const startDate = new Date(finalStartDate);
          const endDate = new Date(finalEndDate);
          const daysDiff = Math.ceil((endDate - startDate) / (1000 * 60 * 60 * 24)) + 1;
          influxRange = `${daysDiff}d`;
          console.log(`ğŸ“Š InfluxDB-only mode: using ${influxRange} range for custom dates ${finalStartDate} to ${finalEndDate}`);
        }

        // Use the existing getDashboardAnalytics method
        const hourlyData = await influx.getDashboardAnalytics(influxRange, writerId);
        console.log(`ğŸ“Š InfluxDB returned ${hourlyData.length} daily data points`);

        // Filter InfluxDB data using the calculated start date
        const influxFilterStartDate = useBigQuery ?
          // If using both sources, use the calculated InfluxDB start date
          finalInfluxStartDate :
          // If InfluxDB only, use the user's start date
          finalStartDate;

        console.log(`ğŸ“Š InfluxDB date filtering: start=${influxFilterStartDate}, end=${finalEndDate}`);

        hourlyData.forEach(point => {
          const estDate = point.date.toISOString().split('T')[0]; // YYYY-MM-DD format

          // Include dates from InfluxDB start date to end date
          if (estDate >= influxFilterStartDate && estDate <= finalEndDate) {
            influxData.push({
              date: estDate,
              views: point.views,
              source: 'InfluxDB_RealTime'
            });
          }
        });

        // Sort by date
        influxData.sort((a, b) => new Date(a.date) - new Date(b.date));

        console.log(`ğŸ“Š InfluxDB daily aggregates (EST, filtered):`, influxData);

      } catch (err) {
        console.error('âŒ InfluxDB error in getBigQueryAnalyticsOverview:', err.message);
        console.error('âŒ InfluxDB error stack:', err.stack);
      }
    }

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 6) Transform DAILY TOTALS data for frontend â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    // Use DAILY TOTALS BigQuery data for chart (solid lines)
    // FORMAT_DATE returns a string, no timezone conversion needed!
    const dailyTotalsData = dailyTotalsRows.map(row => ({
      time: row.date_string, // FORMAT_DATE string, no timezone issues!
      views: parseInt(row.total_views || 0),
      unique_videos: parseInt(row.unique_videos || 0),
      source: 'BigQuery'
    }));

    // Add InfluxDB real-time data (dotted lines)
    influxData.forEach(item => {
      dailyTotalsData.push({
        time: item.date,
        views: item.views,
        source: 'InfluxDB'
      });
    });

    // Sort by date
    dailyTotalsData.sort((a, b) => new Date(a.time) - new Date(b.time));

    // Calculate chart data for frontend - DAILY TOTALS data points
    const chartData = dailyTotalsData.map(item => ({
      date: item.time,
      views: item.views,
      formattedDate: new Date(item.time).toLocaleDateString(),
      unique_videos: item.unique_videos || 0,
      source: item.source
    }));

    // Calculate final total views including fallback data
    const finalTotalViews = dailyTotalsData.reduce((sum, item) => sum + item.views, 0);

    console.log(`ğŸ“Š Analytics Overview complete: ${dailyTotalsData.length} data points`);
    console.log(`ğŸ“Š Final total views (BigQuery + InfluxDB): ${finalTotalViews.toLocaleString()}`);
    console.log(`ğŸ“Š BigQuery total views: ${totalViews.toLocaleString()}`);
    console.log(`ğŸ“Š InfluxDB data points: ${influxData.length}`);

    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 8) Return frontend-compatible DAILY TOTALS data â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    return {
      totalViews: finalTotalViews,
      chartData: chartData,
      aggregatedViewsData: dailyTotalsData, // Use daily totals data as QA script shows
      avgDailyViews: dailyTotalsData.length > 0 ? Math.round(finalTotalViews / dailyTotalsData.length) : 0,
      summary: {
        progressToTarget: (finalTotalViews / 100000000) * 100,
        highestDay: dailyTotalsData.length > 0 ? Math.max(...dailyTotalsData.map(d => d.views)) : 0,
        lowestDay: dailyTotalsData.length > 0 ? Math.min(...dailyTotalsData.map(d => d.views)) : 0
      },
      metadata: {
        source: 'BigQuery youtube_video_report_historical (solid lines) + InfluxDB last 3 days (dotted lines)',
        dataSource: 'BigQuery: all historical data + InfluxDB: last 3 days real-time',
        lastUpdated: new Date().toISOString(),
        range: range,
        bigQueryIntegrated: true,
        influxDBIntegrated: true,
        tableUsed: 'youtube_video_report_historical',
        influxDBDays: 3
      },
      // Keep raw data for debugging
      rawViews: rawViewsRows,
      dailyTotals: dailyTotalsRows,
      influxData: influxData
    };
  }
  catch (err) {
    console.error('âŒ Analytics overview error:', err);
    throw err;
  }
}

// Example Express endpoint
router.get('/analytics-overview', async (req, res) => {
  try {
    const { writerId, range, limit } = req.query;
    const result = await getBigQueryAnalyticsOverview(writerId, range, null, parseInt(limit) || 100);
    res.json(result);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});

// Test endpoint without auth for debugging
router.get('/test-overview', async (req, res) => {
  try {
    // Mock user data for testing - find the correct login_id for writer_id=110
    console.log('ğŸ§ª Test overview endpoint called');

    // First, find the login_id for writer_id=110
    const writerQuery = `SELECT login_id FROM writer WHERE id = 110`;
    const writerResult = await pool.query(writerQuery);

    let userId = 1; // default fallback
    if (writerResult.rows.length > 0) {
      userId = writerResult.rows[0].login_id;
      console.log(`âœ… Found login_id ${userId} for writer_id 110`);
    } else {
      console.log('âš ï¸ No login_id found for writer_id 110, using default user id 1');
    }

    req.user = { id: userId };
    req.query = { range: '30d', ...req.query };

    // Call the main analytics logic
    return await handleAnalyticsRequest(req, res);
  } catch (error) {
    console.error('âŒ Test overview error:', error);
    res.status(500).json({ message: 'Test overview error', error: error.message });
  }
});

// Direct test endpoint for writer_id=110
router.get('/test-writer-110', async (req, res) => {
  try {
    console.log('ğŸ§ª Direct test for writer_id=110');

    const writerId = 110;
    const range = '30d';

    // Get writer name
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);
    const writerName = writerRows[0]?.name || 'Unknown Writer';

    console.log(`ğŸ“ Testing with writer: ${writerName} (ID: ${writerId})`);

    // Call our analytics function directly
    const analyticsData = await getBigQueryAnalyticsOverview(writerId, range, writerName);

    console.log('âœ… Direct test successful:', {
      totalViews: analyticsData.totalViews,
      chartDataPoints: analyticsData.chartData?.length || 0,
      aggregatedViewsDataPoints: analyticsData.aggregatedViewsData?.length || 0
    });

    res.json(analyticsData);
  } catch (error) {
    console.error('âŒ Direct test error:', error);
    res.status(500).json({ message: 'Direct test error', error: error.message, stack: error.stack });
  }
});

// Debug endpoint to check raw InfluxDB data
router.get('/debug-influx-raw', async (req, res) => {
  try {
    console.log('ğŸ” DEBUG: Checking raw InfluxDB data');

    const writerId = 110;
    const range = '7d'; // Use shorter range for debugging

    // Get writer name
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);
    const writerName = writerRows[0]?.name || 'Unknown Writer';

    console.log(`ğŸ” DEBUG: Writer: ${writerName} (ID: ${writerId})`);

    // Check if InfluxDB service is available
    const InfluxService = require('../services/influxService');
    const influxService = new InfluxService();

    if (!influxService) {
      return res.json({ error: 'InfluxDB service not available' });
    }

    console.log(`ğŸ” DEBUG: Getting InfluxDB data for range: ${range}`);

    // Get both total views and daily analytics
    const [totalViews, dailyAnalytics] = await Promise.all([
      influxService.getTotalViews(range, writerId),
      influxService.getDashboardAnalytics(range, writerId)
    ]);

    console.log(`ğŸ” DEBUG: InfluxDB total views: ${totalViews}`);
    console.log(`ğŸ” DEBUG: InfluxDB daily analytics: ${dailyAnalytics.length} records`);

    // Transform daily analytics for debugging
    const transformedDaily = dailyAnalytics.map(day => ({
      date: new Date(day.date).toISOString().split('T')[0],
      views: day.views,
      rawDate: day.date
    }));

    console.log(`ğŸ” DEBUG: Sample daily data:`, transformedDaily.slice(0, 3));

    // Calculate total from daily data
    const calculatedTotal = transformedDaily.reduce((sum, day) => sum + day.views, 0);

    res.json({
      success: true,
      writer: { id: writerId, name: writerName },
      range: range,
      influxTotalViews: totalViews,
      dailyAnalyticsCount: dailyAnalytics.length,
      dailyAnalytics: transformedDaily,
      calculatedTotalFromDaily: calculatedTotal,
      discrepancy: Math.abs(totalViews - calculatedTotal),
      sampleRawDaily: dailyAnalytics.slice(0, 3)
    });

  } catch (error) {
    console.error('ğŸ” DEBUG: Error in InfluxDB debug endpoint:', error);
    res.status(500).json({
      error: 'InfluxDB debug endpoint error',
      message: error.message,
      stack: error.stack
    });
  }
});

// Debug endpoint to check raw BigQuery data
router.get('/debug-bigquery-raw', async (req, res) => {
  try {
    console.log('ğŸ” DEBUG: Checking raw BigQuery data');

    const writerId = 110;
    const range = '7d'; // Use shorter range for debugging

    // Get writer name
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);
    const writerName = writerRows[0]?.name || 'Unknown Writer';

    console.log(`ğŸ” DEBUG: Writer: ${writerName} (ID: ${writerId})`);

    if (!bigquery) {
      return res.json({ error: 'BigQuery client not initialized' });
    }

    // Calculate date range
    const today = new Date();
    const endDate = today.toISOString().split('T')[0];
    const startDate = new Date(today);
    startDate.setDate(today.getDate() - 7);
    const startDateStr = startDate.toISOString().split('T')[0];

    console.log(`ğŸ” DEBUG: Date range: ${startDateStr} to ${endDate}`);

    // Raw BigQuery query
    const projectId = process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";
    const analyticsDataset = process.env.BIGQUERY_ANALYTICS_DATASET || "dbt_youtube_analytics";
    const analyticsTable = process.env.BIGQUERY_ANALYTICS_TABLE || "youtube_metadata_historical";

    const bigQueryQuery = `
      SELECT
        snapshot_date AS time,
        SUM(CAST(statistics_view_count AS INT64)) AS views
      FROM \`${projectId}.${analyticsDataset}.${analyticsTable}\`
      WHERE writer_id = @writer_id
        AND DATE(snapshot_date) BETWEEN @start_date AND @end_date
        AND writer_id IS NOT NULL
        AND statistics_view_count IS NOT NULL
      GROUP BY snapshot_date
      ORDER BY snapshot_date ASC;
    `;

    console.log(`ğŸ” DEBUG: Executing BigQuery query:`, bigQueryQuery);
    console.log(`ğŸ” DEBUG: Query params:`, { writer_id: writerId, start_date: startDateStr, end_date: endDate });

    const [bigQueryRows] = await bigquery.query({
      query: bigQueryQuery,
      params: {
        writer_id: parseInt(writerId),
        start_date: startDateStr,
        end_date: endDate
      }
    });

    console.log(`ğŸ” DEBUG: BigQuery returned ${bigQueryRows.length} rows`);

    // Show raw data
    // FIX: Convert BigQuery date to string to prevent timezone shifts
    const rawData = bigQueryRows.map(row => ({
      date: row.time.value instanceof Date ?
        row.time.value.toISOString().split('T')[0] :
        row.time.value,
      absoluteViews: parseInt(row.views || 0)
    }));

    console.log(`ğŸ” DEBUG: Raw BigQuery data:`, rawData);

    // Calculate daily increases
    const dailyIncreases = [];
    for (let i = 0; i < rawData.length; i++) {
      const currentDay = rawData[i];
      let dailyIncrease = 0;

      if (i === 0) {
        // First day - use absolute count as baseline
        dailyIncrease = currentDay.absoluteViews;
        console.log(`ğŸ” DEBUG: ${currentDay.date} baseline: ${dailyIncrease} views`);
      } else {
        // Subsequent days - calculate increase
        const previousDay = rawData[i - 1];
        dailyIncrease = currentDay.absoluteViews - previousDay.absoluteViews;
        console.log(`ğŸ” DEBUG: ${currentDay.date}: ${currentDay.absoluteViews} - ${previousDay.absoluteViews} = ${dailyIncrease} increase`);
      }

      dailyIncreases.push({
        date: currentDay.date,
        absoluteViews: currentDay.absoluteViews,
        dailyIncrease: Math.max(0, dailyIncrease)
      });
    }

    const totalViews = dailyIncreases.reduce((sum, day) => sum + day.dailyIncrease, 0);

    console.log(`ğŸ” DEBUG: Total views calculated: ${totalViews.toLocaleString()}`);

    res.json({
      success: true,
      writer: { id: writerId, name: writerName },
      dateRange: { start: startDateStr, end: endDate },
      rawBigQueryRows: bigQueryRows.length,
      rawData: rawData,
      dailyIncreases: dailyIncreases,
      totalViews: totalViews,
      avgDailyViews: dailyIncreases.length > 0 ? Math.round(totalViews / dailyIncreases.length) : 0
    });

  } catch (error) {
    console.error('ğŸ” DEBUG: Error in debug endpoint:', error);
    res.status(500).json({
      error: 'Debug endpoint error',
      message: error.message,
      stack: error.stack
    });
  }
});

// Debug endpoint to compare all data sources
router.get('/debug-data-comparison', async (req, res) => {
  try {
    console.log('ğŸ” DEBUG: Comparing all data sources');

    const writerId = 110;
    const range = '7d';

    // Get writer name
    const writerQuery = `SELECT name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);
    const writerName = writerRows[0]?.name || 'Unknown Writer';

    console.log(`ğŸ” DEBUG: Comparing data for writer: ${writerName} (ID: ${writerId})`);

    const results = {
      writer: { id: writerId, name: writerName },
      range: range,
      influxData: null,
      bigQueryData: null,
      combinedData: null,
      errors: []
    };

    // 1. Test InfluxDB
    try {
      const InfluxService = require('../services/influxService');
      const influxService = new InfluxService();

      const [influxTotal, influxDaily] = await Promise.all([
        influxService.getTotalViews(range, writerId),
        influxService.getDashboardAnalytics(range, writerId)
      ]);

      const influxTransformed = influxDaily.map(day => ({
        date: new Date(day.date).toISOString().split('T')[0],
        views: day.views
      }));

      results.influxData = {
        totalViews: influxTotal,
        dailyCount: influxDaily.length,
        dailyData: influxTransformed,
        calculatedTotal: influxTransformed.reduce((sum, day) => sum + day.views, 0)
      };

      console.log(`ğŸ” DEBUG: InfluxDB - Total: ${influxTotal}, Daily records: ${influxDaily.length}`);

    } catch (influxError) {
      console.error('ğŸ” DEBUG: InfluxDB error:', influxError);
      results.errors.push(`InfluxDB: ${influxError.message}`);
    }

    // 2. Test BigQuery
    try {
      if (bigquery) {
        const today = new Date();
        const endDate = today.toISOString().split('T')[0];
        const startDate = new Date(today);
        startDate.setDate(today.getDate() - 7);
        const startDateStr = startDate.toISOString().split('T')[0];

        const projectId = process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";
        const analyticsDataset = process.env.BIGQUERY_ANALYTICS_DATASET || "dbt_youtube_analytics";
        const analyticsTable = process.env.BIGQUERY_ANALYTICS_TABLE || "youtube_metadata_historical";

        const bigQueryQuery = `
          SELECT
            snapshot_date AS time,
            SUM(CAST(statistics_view_count AS INT64)) AS views
          FROM \`${projectId}.${analyticsDataset}.${analyticsTable}\`
          WHERE writer_id = @writer_id
            AND DATE(snapshot_date) BETWEEN @start_date AND @end_date
            AND writer_id IS NOT NULL
            AND statistics_view_count IS NOT NULL
          GROUP BY snapshot_date
          ORDER BY snapshot_date ASC;
        `;

        const [bigQueryRows] = await bigquery.query({
          query: bigQueryQuery,
          params: {
            writer_id: parseInt(writerId),
            start_date: startDateStr,
            end_date: endDate
          }
        });

        const bigQueryRaw = bigQueryRows.map(row => ({
          date: row.time.value instanceof Date ?
            row.time.value.toISOString().split('T')[0] :
            row.time.value,
          absoluteViews: parseInt(row.views || 0)
        }));

        // Calculate daily increases
        const bigQueryIncreases = [];
        for (let i = 0; i < bigQueryRaw.length; i++) {
          const currentDay = bigQueryRaw[i];
          let dailyIncrease = 0;

          if (i === 0) {
            dailyIncrease = currentDay.absoluteViews; // This is the problem!
          } else {
            const previousDay = bigQueryRaw[i - 1];
            dailyIncrease = currentDay.absoluteViews - previousDay.absoluteViews;
          }

          bigQueryIncreases.push({
            date: currentDay.date,
            absoluteViews: currentDay.absoluteViews,
            dailyIncrease: Math.max(0, dailyIncrease)
          });
        }

        results.bigQueryData = {
          rawCount: bigQueryRows.length,
          rawData: bigQueryRaw,
          increasesData: bigQueryIncreases,
          totalFromIncreases: bigQueryIncreases.reduce((sum, day) => sum + day.dailyIncrease, 0),
          firstDayProblem: bigQueryIncreases.length > 0 ? bigQueryIncreases[0].dailyIncrease : 0
        };

        console.log(`ğŸ” DEBUG: BigQuery - Raw records: ${bigQueryRows.length}, First day increase: ${results.bigQueryData.firstDayProblem}`);

      } else {
        results.errors.push('BigQuery: Client not initialized');
      }

    } catch (bigQueryError) {
      console.error('ğŸ” DEBUG: BigQuery error:', bigQueryError);
      results.errors.push(`BigQuery: ${bigQueryError.message}`);
    }

    // 3. Test combined analytics function
    try {
      const analyticsData = await getBigQueryAnalyticsOverview(writerId, range, writerName);

      results.combinedData = {
        totalViews: analyticsData.totalViews,
        chartDataCount: analyticsData.chartData?.length || 0,
        aggregatedDataCount: analyticsData.aggregatedViewsData?.length || 0,
        sampleChartData: analyticsData.chartData?.slice(0, 3) || [],
        sampleAggregatedData: analyticsData.aggregatedViewsData?.slice(0, 3) || []
      };

      console.log(`ğŸ” DEBUG: Combined function - Total: ${analyticsData.totalViews}, Chart points: ${analyticsData.chartData?.length}`);

    } catch (combinedError) {
      console.error('ğŸ” DEBUG: Combined function error:', combinedError);
      results.errors.push(`Combined: ${combinedError.message}`);
    }

    res.json(results);

  } catch (error) {
    console.error('ğŸ” DEBUG: Error in data comparison endpoint:', error);
    res.status(500).json({
      error: 'Data comparison endpoint error',
      message: error.message,
      stack: error.stack
    });
  }
});

// Quick data analysis endpoint for past 30 days including June 5th
router.get('/debug-30days-june5', async (req, res) => {
  try {
    console.log('ğŸ” DEBUG: Analyzing 30 days including June 5th');

    const writerId = 110;
    const endDate = '2025-06-05'; // June 5th
    const startDate = '2025-05-06'; // 30 days before

    // Get writer info
    const writerQuery = `SELECT id, name, login_id FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);

    if (writerRows.length === 0) {
      return res.json({ error: `Writer ${writerId} not found` });
    }

    const writer = writerRows[0];
    console.log(`ğŸ“ Analyzing writer: ${writer.name} (ID: ${writer.id})`);
    console.log(`ğŸ“… Date range: ${startDate} to ${endDate}`);

    const results = {
      writer: writer,
      dateRange: { start: startDate, end: endDate },
      influxData: null,
      bigQueryData: null,
      errors: []
    };

    // 1. InfluxDB Data
    try {
      const InfluxService = require('../services/influxService');
      const influxService = new InfluxService();

      // Calculate time range for InfluxDB
      const daysDiff = Math.ceil((new Date(endDate) - new Date(startDate)) / (1000 * 60 * 60 * 24));
      const timeRange = `${daysDiff + 5}d`;

      console.log(`ğŸ” InfluxDB time range: ${timeRange}`);

      const [influxTotal, influxDaily] = await Promise.all([
        influxService.getTotalViews(timeRange, writerId),
        influxService.getDashboardAnalytics(timeRange, writerId)
      ]);

      // Filter to exact date range and transform
      const filteredDaily = influxDaily
        .map(day => ({
          date: new Date(day.date).toISOString().split('T')[0],
          views: day.views,
          rawDate: day.date
        }))
        .filter(day => day.date >= startDate && day.date <= endDate)
        .sort((a, b) => new Date(a.date) - new Date(b.date));

      const calculatedTotal = filteredDaily.reduce((sum, day) => sum + day.views, 0);

      results.influxData = {
        totalViews: influxTotal,
        dailyRecords: filteredDaily.length,
        dailyData: filteredDaily.slice(0, 10), // First 10 days for display
        calculatedTotal: calculatedTotal,
        avgDaily: filteredDaily.length > 0 ? Math.round(calculatedTotal / filteredDaily.length) : 0,
        firstDay: filteredDaily[0] || null,
        lastDay: filteredDaily[filteredDaily.length - 1] || null
      };

      console.log(`ğŸ“Š InfluxDB: ${influxTotal.toLocaleString()} total, ${filteredDaily.length} daily records`);

    } catch (influxError) {
      console.error('âŒ InfluxDB error:', influxError);
      results.errors.push(`InfluxDB: ${influxError.message}`);
    }

    // 2. BigQuery Data
    try {
      if (bigquery) {
        const projectId = process.env.BIGQUERY_PROJECT_ID || "speedy-web-461014-g3";
        const dataset = "dbt_youtube_analytics";
        const table = "youtube_metadata_historical";

        const bigQueryQuery = `
          SELECT
            snapshot_date AS time,
            SUM(CAST(statistics_view_count AS INT64)) AS views
          FROM \`${projectId}.${dataset}.${table}\`
          WHERE writer_id = @writer_id
            AND DATE(snapshot_date) BETWEEN @start_date AND @end_date
            AND writer_id IS NOT NULL
            AND statistics_view_count IS NOT NULL
          GROUP BY snapshot_date
          ORDER BY snapshot_date ASC;
        `;

        console.log(`ğŸ” BigQuery query for writer ${writerId}, ${startDate} to ${endDate}`);

        const [bigQueryRows] = await bigquery.query({
          query: bigQueryQuery,
          params: {
            writer_id: parseInt(writerId),
            start_date: startDate,
            end_date: endDate
          }
        });

        const bigQueryRaw = bigQueryRows.map(row => ({
          date: row.time.value instanceof Date ?
            row.time.value.toISOString().split('T')[0] :
            row.time.value,
          absoluteViews: parseInt(row.views || 0)
        }));

        // Calculate daily increases (showing the problem)
        const bigQueryIncreases = [];
        for (let i = 0; i < bigQueryRaw.length; i++) {
          const currentDay = bigQueryRaw[i];
          let dailyIncrease = 0;

          if (i === 0) {
            // THIS IS THE PROBLEM!
            dailyIncrease = currentDay.absoluteViews;
            console.log(`ğŸš¨ FIRST DAY PROBLEM: Using ${currentDay.absoluteViews.toLocaleString()} as daily increase!`);
          } else {
            const previousDay = bigQueryRaw[i - 1];
            dailyIncrease = currentDay.absoluteViews - previousDay.absoluteViews;
          }

          bigQueryIncreases.push({
            date: currentDay.date,
            absoluteViews: currentDay.absoluteViews,
            dailyIncrease: Math.max(0, dailyIncrease),
            isFirstDay: i === 0
          });
        }

        const totalFromIncreases = bigQueryIncreases.reduce((sum, day) => sum + day.dailyIncrease, 0);

        results.bigQueryData = {
          rawRecords: bigQueryRows.length,
          rawData: bigQueryRaw.slice(0, 5), // First 5 for display
          increasesData: bigQueryIncreases.slice(0, 10), // First 10 for display
          totalFromIncreases: totalFromIncreases,
          firstDayProblem: bigQueryIncreases.length > 0 ? bigQueryIncreases[0].dailyIncrease : 0,
          firstDay: bigQueryIncreases[0] || null,
          lastDay: bigQueryIncreases[bigQueryIncreases.length - 1] || null
        };

        console.log(`ğŸ“Š BigQuery: ${bigQueryRows.length} records, total from increases: ${totalFromIncreases.toLocaleString()}`);
        console.log(`ğŸš¨ First day problem: ${results.bigQueryData.firstDayProblem.toLocaleString()}`);

      } else {
        results.errors.push('BigQuery: Not available');
      }

    } catch (bigQueryError) {
      console.error('âŒ BigQuery error:', bigQueryError);
      results.errors.push(`BigQuery: ${bigQueryError.message}`);
    }

    // Summary
    if (results.influxData && results.bigQueryData) {
      results.comparison = {
        influxTotal: results.influxData.calculatedTotal,
        bigQueryTotal: results.bigQueryData.totalFromIncreases,
        difference: Math.abs(results.influxData.calculatedTotal - results.bigQueryData.totalFromIncreases),
        ratio: results.bigQueryData.totalFromIncreases / results.influxData.calculatedTotal,
        firstDayIssue: results.bigQueryData.firstDayProblem
      };

      console.log(`ğŸ” COMPARISON:`);
      console.log(`   InfluxDB: ${results.comparison.influxTotal.toLocaleString()}`);
      console.log(`   BigQuery: ${results.comparison.bigQueryTotal.toLocaleString()}`);
      console.log(`   Ratio: ${results.comparison.ratio.toFixed(2)}x`);
      console.log(`   First Day Issue: ${results.comparison.firstDayIssue.toLocaleString()}`);
    }

    res.json(results);

  } catch (error) {
    console.error('ğŸ” DEBUG: Error in 30-day analysis:', error);
    res.status(500).json({
      error: '30-day analysis error',
      message: error.message
    });
  }
});

// Get analytics data with BigQuery
router.get('/', authenticateToken, async (req, res) => {
  return await handleAnalyticsRequest(req, res);
});

// Main analytics logic (extracted for reuse)
async function handleAnalyticsRequest(req, res) {
  try {
    console.log('ğŸ”¥ OVERVIEW ENDPOINT CALLED! Query params:', req.query);
    let { range = '30d', start_date, end_date } = req.query;

    // Check if this is a custom date range
    let customStartDate = null;
    let customEndDate = null;

    if (range.startsWith('custom_')) {
      // Parse custom date range from format: custom_2025-06-03_2025-06-06
      const parts = range.split('_');
      if (parts.length === 3) {
        customStartDate = parts[1];
        customEndDate = parts[2];
        range = 'custom';
        console.log(`ğŸ“… Parsed custom date range: ${customStartDate} to ${customEndDate}`);
      }
    } else if (start_date && end_date) {
      // Handle direct start_date and end_date parameters
      customStartDate = start_date;
      customEndDate = end_date;
      range = 'custom';
      console.log(`ğŸ“… Using direct custom date range: ${customStartDate} to ${customEndDate}`);
    }

    // Enhanced frontend time ranges mapping with dynamic date calculation
    const timeRangeMap = {
      'last7days': '7d',
      'last30days': '30d',
      'last90days': '90d',
      'last365days': '365d',
      'lifetime': 'lifetime',
      '2025': 'year_2025',
      '2024': 'year_2024',
      'may': 'month_may',
      'april': 'month_april',
      'march': 'march_march',
      '7d': '7d',
      '30d': '30d',
      '90d': '90d',
      'custom': 'custom'
    };

    range = timeRangeMap[range] || '30d';
    const userId = req.user.id;

    console.log('ğŸ“Š Getting analytics for user ID:', userId, 'Range:', range);

    // Get writer information from PostgreSQL (get both ID and name)
    let writerId = null;
    let writerName = null;
    try {
      const writerQuery = `
        SELECT w.id as writer_id, w.name as writer_name
        FROM writer w
        WHERE w.login_id = $1
      `;
      const writerResult = await pool.query(writerQuery, [userId]);
      if (writerResult.rows.length > 0) {
        writerId = writerResult.rows[0].writer_id;
        writerName = writerResult.rows[0].writer_name;
        console.log('âœ… Found writer:', { id: writerId, name: writerName }, 'for user:', userId);
      } else {
        console.log('âš ï¸ No writer found for user:', userId);
      }
    } catch (dbError) {
      console.error('âŒ Error getting writer info:', dbError);
    }

    if (writerId) {
      try {
        console.log('ğŸ”¥ About to call getBigQueryAnalyticsOverview with params:', {
          writerId,
          range,
          writerName,
          limit: 100,
          customStartDate,
          customEndDate
        });

        // Use BigQuery for analytics overview with writer name from PostgreSQL
        const analyticsData = await getBigQueryAnalyticsOverview(
          writerId,
          range,
          writerName,
          100,
          customStartDate,
          customEndDate
        );

        console.log('ğŸ“Š BigQuery analytics data sent:', {
          totalViews: analyticsData.totalViews,
          totalSubmissions: analyticsData.totalSubmissions,
          topVideosCount: analyticsData.topVideos?.length || 0,
          hasLatestContent: !!analyticsData.latestContent,
          range: analyticsData.range,
          customDateRange: customStartDate && customEndDate ? `${customStartDate} to ${customEndDate}` : null
        });

        res.json(analyticsData);
        return;

      } catch (bigQueryError) {
        console.error('âŒ BigQuery error in analytics overview:', bigQueryError);
        console.error('âŒ BigQuery error details:', bigQueryError.message);
        console.error('âŒ BigQuery error stack:', bigQueryError.stack);

        // Return error response instead of fallback
        return res.status(500).json({
          error: 'BigQuery analytics data unavailable',
          message: 'Unable to fetch analytics data from BigQuery',
          details: bigQueryError.message
        });
      }
    }

    // Fallback to dummy data if InfluxDB fails
    const dummyData = {
      // Don't include totalViews here - let channel endpoint handle it
      totalSubmissions: 15,
      acceptedSubmissions: 8,
      rejectedSubmissions: 4,
      pendingSubmissions: 3,
      acceptanceRate: 53.3,
      monthlySubmissions: [
        { month: 'Jan', submissions: 2, accepted: 1 },
        { month: 'Feb', submissions: 3, accepted: 2 },
        { month: 'Mar', submissions: 4, accepted: 2 },
        { month: 'Apr', submissions: 6, accepted: 3 }
      ],
      submissionsByType: [
        { type: 'Trope', count: 8 },
        { type: 'Original', count: 5 },
        { type: 'TLDR', count: 2 }
      ],
      recentActivity: [
        { date: '2025-04-20', action: 'Submission created', title: '[STL] test test do not edit this' },
        { date: '2025-04-15', action: 'Submission created', title: '[Original] My family has a death' },
        { date: '2025-03-16', action: 'Submission accepted', title: '[STL] My boy best friend thinks...' },
        { date: '2025-03-01', action: 'Submission rejected', title: '[STL] testing 123' }
      ]
    };

    res.json(dummyData);
  } catch (error) {
    console.error('âŒ Analytics endpoint error:', error);
    res.status(500).json({ message: 'Server error' });
  }
}

// Helper function to generate monthly statistics
function generateMonthlyStats(submissions) {
  const monthlyData = {};
  const months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'];

  submissions.forEach(submission => {
    const date = new Date(submission.submittedOn);
    const monthKey = months[date.getMonth()];

    if (!monthlyData[monthKey]) {
      monthlyData[monthKey] = { month: monthKey, submissions: 0, accepted: 0 };
    }

    monthlyData[monthKey].submissions++;
    if (submission.status === 'Posted') {
      monthlyData[monthKey].accepted++;
    }
  });

  return Object.values(monthlyData);
}

// Get submission statistics by date range
router.get('/stats', authenticateToken, (req, res) => {
  try {
    const { startDate, endDate } = req.query;

    // Dummy filtered statistics
    const stats = {
      period: `${startDate} to ${endDate}`,
      totalSubmissions: 8,
      acceptedSubmissions: 4,
      rejectedSubmissions: 2,
      pendingSubmissions: 2,
      averageResponseTime: '5.2 days',
      topPerformingTypes: [
        { type: 'Trope', acceptanceRate: 60 },
        { type: 'Original', acceptanceRate: 40 },
        { type: 'TLDR', acceptanceRate: 50 }
      ]
    };

    res.json(stats);
  } catch (error) {
    res.status(500).json({ message: 'Server error' });
  }
});

// Get channel analytics data with real InfluxDB data
router.get('/channel', authenticateToken, async (req, res) => {
  try {
    const { range = 'lifetime' } = req.query;
    console.log('ğŸ”¥ CHANNEL ENDPOINT CALLED! Query params:', req.query);
    console.log('ğŸ”¥ User from token:', req.user);

    // Convert frontend range to InfluxDB range
    const convertRange = (range) => {
      switch (range) {
        case 'last7days': return '7d';
        case 'last28days': return '28d';
        case 'last30days': return '30d';
        case 'last90days': return '90d';
        case 'last365days': return '365d';
        case 'lifetime': return '3y'; // Use 3 years for lifetime
        case '2025': return '150d';
        case '2024': return '365d';
        case 'may': return '31d';
        case 'april': return '30d';
        case 'march': return '31d';
        default: return '30d';
      }
    };

    const influxRange = convertRange(range);
    console.log('ğŸ”¥ Converted range:', range, '->', influxRange);

    // Get writer information from PostgreSQL
    let writerId = null;
    const userId = req.user.id;
    console.log('ğŸ”¥ Looking up writer for user ID:', userId);

    try {

      const writerQuery = `
        SELECT w.id as writer_id, w.name as writer_name
        FROM writer w
        WHERE w.login_id = $1
      `;
      console.log('ğŸ”¥ Executing writer query:', writerQuery, 'with user ID:', userId);
      const writerResult = await pool.query(writerQuery, [userId]);
      console.log('ğŸ”¥ Writer query result:', writerResult.rows);

      if (writerResult.rows.length > 0) {
        writerId = writerResult.rows[0].writer_id;
        console.log('âœ… Found writer ID:', writerId, 'for user:', userId);
      } else {
        console.log('âš ï¸ No writer found for user:', userId);
        console.log('ğŸ” Let me check what users exist in the login table...');

        // Debug: Check what users exist
        const debugQuery = 'SELECT id, username FROM login LIMIT 10';
        const debugResult = await pool.query(debugQuery);
        console.log('ğŸ” Available users in login table:', debugResult.rows);

        // Debug: Check what writers exist
        const writerDebugQuery = 'SELECT id, login_id, name FROM writer LIMIT 10';
        const writerDebugResult = await pool.query(writerDebugQuery);
        console.log('ğŸ” Available writers:', writerDebugResult.rows);
      }
    } catch (dbError) {
      console.error('âŒ Error getting writer ID for channel analytics:', dbError);
    }

    // Get real data from BigQuery for chart data and views, InfluxDB for top videos
    let totalViews = 0;
    let chartData = [];
    let dataSource = 'BigQuery - Real Data';
    let hasDataIssues = false;

    console.log(`ğŸ” DEBUGGING: Checking BigQuery condition: writerId=${writerId}, bigquery=${!!bigquery}`);

    if (writerId && bigquery) {
      try {
        console.log(`ğŸ” Getting BigQuery data for writer ${writerId}, range: ${influxRange}`);

        // Convert range to start/end dates for BigQuery
        const endDate = new Date();
        const startDate = new Date();

        switch (influxRange) {
          case '7d':
            startDate.setDate(endDate.getDate() - 7);
            break;
          case '28d':
            startDate.setDate(endDate.getDate() - 28);
            break;
          case '30d':
            startDate.setDate(endDate.getDate() - 30);
            break;
          case '90d':
            startDate.setDate(endDate.getDate() - 90);
            break;
          case '365d':
            startDate.setDate(endDate.getDate() - 365);
            break;
          case '3y':
            startDate.setFullYear(endDate.getFullYear() - 3);
            break;
          default:
            startDate.setDate(endDate.getDate() - 30);
        }

        const startDateStr = startDate.toISOString().split('T')[0];
        const endDateStr = endDate.toISOString().split('T')[0];

        console.log(`ğŸ“Š BigQuery date range: ${startDateStr} to ${endDateStr}`);

        // Get chart data from BigQuery
        const bigQueryData = await getBigQueryViews(writerId, startDateStr, endDateStr, influxService);

        // Transform BigQuery data for chart
        // FIX: Convert BigQuery date to string to prevent timezone shifts
        chartData = bigQueryData.map(row => ({
          date: row.time.value instanceof Date ?
            row.time.value.toISOString().split('T')[0] :
            row.time.value,
          views: row.views,
          timestamp: new Date(row.time.value instanceof Date ?
            row.time.value.toISOString().split('T')[0] :
            row.time.value).getTime()
        })).sort((a, b) => a.timestamp - b.timestamp);

        // Calculate total views
        totalViews = chartData.reduce((sum, day) => sum + day.views, 0);

        console.log(`ğŸ“Š BigQuery data processed: ${totalViews.toLocaleString()} total views, ${chartData.length} chart points`);
        console.log(`ğŸ“Š Sample chart data:`, chartData.slice(0, 3));

      } catch (bigQueryError) {
        console.error('âŒ BigQuery error for chart data:', bigQueryError);
        hasDataIssues = true;
        dataSource = 'BigQuery Error';

        // Generate minimal fallback data to prevent UI crashes
        const today = new Date();
        chartData = Array.from({ length: 7 }, (_, i) => {
          const date = new Date(today);
          date.setDate(date.getDate() - (6 - i));
          return {
            date: date.toISOString().split('T')[0],
            views: 0,
            timestamp: date.getTime()
          };
        });
        totalViews = 0;
      }
    } else {
      // If BigQuery is not available, use InfluxDB directly
      console.log(`ğŸ”„ BigQuery not available, using InfluxDB directly for writer ${writerId}`);
      if (influxService && writerId) {
        try {
          const [influxTotalViews, influxTotalLikes, influxTotalComments, dailyAnalytics] = await Promise.all([
            influxService.getTotalViews(influxRange, writerId),
            influxService.getTotalLikes(influxRange, writerId),
            influxService.getTotalComments(influxRange, writerId),
            influxService.getDashboardAnalytics(influxRange, writerId)
          ]);

          totalViews = influxTotalViews;
          totalLikes = influxTotalLikes;
          totalComments = influxTotalComments;

          chartData = dailyAnalytics
            .map(day => {
              // Convert InfluxDB UTC time to EST
              const utcDate = new Date(day.date);
              const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
              return {
                date: estDate.toISOString().split('T')[0],
                views: Math.round(day.views),
                timestamp: estDate.getTime()
              };
            })
            .sort((a, b) => a.timestamp - b.timestamp)
            .filter(day => day.views > 0);

          dataSource = 'InfluxDB - Real Data';
          console.log(`ğŸ“Š InfluxDB data processed: ${totalViews.toLocaleString()} total views, ${totalLikes.toLocaleString()} total likes, ${totalComments.toLocaleString()} total comments, ${chartData.length} chart points`);
          console.log(`ğŸ“Š Sample chart data:`, chartData.slice(0, 3));
        } catch (influxError) {
          console.error('âŒ InfluxDB error:', influxError);
          hasDataIssues = true;
          dataSource = 'No Data Available';
        }
      }
    }

    // Get top videos from BigQuery first, then fallback to InfluxDB
    let topVideos = [];
    if (writerId) {
      try {
        // Try BigQuery first for top videos
        if (bigquery) {
          console.log(`ğŸ† Getting top videos from BigQuery for writer ${writerId}`);

          // Calculate date range for top videos (same as chart data)
          const topVideosEndDate = new Date();
          const topVideosStartDate = new Date();

          switch (influxRange) {
            case '7d':
              topVideosStartDate.setDate(topVideosEndDate.getDate() - 7);
              break;
            case '28d':
              topVideosStartDate.setDate(topVideosEndDate.getDate() - 28);
              break;
            case '30d':
              topVideosStartDate.setDate(topVideosEndDate.getDate() - 30);
              break;
            case '90d':
              topVideosStartDate.setDate(topVideosEndDate.getDate() - 90);
              break;
            case '365d':
              topVideosStartDate.setDate(topVideosEndDate.getDate() - 365);
              break;
            case '3y':
              topVideosStartDate.setFullYear(topVideosEndDate.getFullYear() - 3);
              break;
            default:
              topVideosStartDate.setDate(topVideosEndDate.getDate() - 30);
          }

          const topVideosStartDateStr = topVideosStartDate.toISOString().split('T')[0];
          const topVideosEndDateStr = topVideosEndDate.toISOString().split('T')[0];

          topVideos = await getBigQueryTopVideos(writerId, topVideosStartDateStr, topVideosEndDateStr, 10);
          console.log(`ğŸ† BigQuery top videos for writer ${writerId}: ${topVideos.length} records`);
        }

        // Fallback to InfluxDB if BigQuery fails or returns no data
        if (topVideos.length === 0 && influxService) {
          console.log(`ğŸ”„ Fallback: Getting top videos from InfluxDB for writer ${writerId}`);
          const allTopVideos = await influxService.getTopVideos(influxRange, 20);

          // Filter by writer and take top 10, then transform to match BigQuery format
          const influxTopVideos = allTopVideos
            .filter(video => video.writer_id == writerId || video.writer_id == writerId.toString())
            .slice(0, 10);

          // Transform InfluxDB data to match VideoAnalytics expected format
          topVideos = influxTopVideos.map((video, index) => ({
            id: video.video_id || `video_${index}`,
            title: video.title || `Video ${video.video_id}`,
            views: parseInt(video.views) || 0,
            likes: Math.floor((video.views || 0) * 0.02), // Estimate 2% like rate
            comments: Math.floor((video.views || 0) * 0.001), // Estimate 0.1% comment rate
            url: video.url,
            preview: video.url ? `https://img.youtube.com/vi/${extractVideoId(video.url)}/maxresdefault.jpg` : "",
            thumbnail: `https://img.youtube.com/vi/${video.video_id}/mqdefault.jpg`,
            posted_date: new Date().toISOString(),
            type: video.url && video.url.includes('/shorts/') ? 'short' : 'video',
            duration: video.url && video.url.includes('/shorts/') ? '0:30' : '5:00',
            engagement: Math.floor(Math.random() * 15) + 85,
            isShort: video.url && video.url.includes('/shorts/'),
            avgViewDuration: video.url && video.url.includes('/shorts/') ? '0:25' : '2:30',
            account_name: video.account_name || 'Unknown Account', // Include account_name from InfluxDB
            writer_name: video.writer_name || 'Unknown Writer' // Include writer_name from InfluxDB
          }));

          console.log(`ğŸ† InfluxDB fallback top videos for writer ${writerId}: ${topVideos.length} records`);
        }

        // If still no data, create mock data to ensure UI works
        if (topVideos.length === 0) {
          console.log(`ğŸ¬ Creating mock top videos data for writer ${writerId}`);
          topVideos = [
            {
              id: 'mock_1',
              title: 'Children of family vloggers, what\'s the reality behind never seen?',
              views: 2316236,
              likes: Math.floor(2316236 * 0.02),
              comments: Math.floor(2316236 * 0.001),
              url: 'https://www.youtube.com/shorts/PaCJ6ZCxAyI',
              preview: 'https://img.youtube.com/vi/PaCJ6ZCxAyI/maxresdefault.jpg',
              thumbnail: 'https://img.youtube.com/vi/PaCJ6ZCxAyI/mqdefault.jpg',
              posted_date: new Date('2025-05-15').toISOString(),
              type: 'short',
              duration: '0:30',
              engagement: 91,
              isShort: true,
              avgViewDuration: '0:25'
            },
            {
              id: 'mock_2',
              title: 'What types of bad parenting do kids not recover from?',
              views: 1561619,
              likes: Math.floor(1561619 * 0.02),
              comments: Math.floor(1561619 * 0.001),
              url: 'https://www.youtube.com/shorts/UdfSPz1LYek',
              preview: 'https://img.youtube.com/vi/UdfSPz1LYek/maxresdefault.jpg',
              thumbnail: 'https://img.youtube.com/vi/UdfSPz1LYek/mqdefault.jpg',
              posted_date: new Date('2025-05-14').toISOString(),
              type: 'short',
              duration: '0:30',
              engagement: 88,
              isShort: true,
              avgViewDuration: '0:25'
            },
            {
              id: 'mock_3',
              title: 'Parents, what\'s the creepiest thing your child has ever said?',
              views: 1219181,
              likes: Math.floor(1219181 * 0.02),
              comments: Math.floor(1219181 * 0.001),
              url: 'https://www.youtube.com/shorts/xyz123',
              preview: 'https://img.youtube.com/vi/xyz123/maxresdefault.jpg',
              thumbnail: 'https://img.youtube.com/vi/xyz123/mqdefault.jpg',
              posted_date: new Date('2025-05-13').toISOString(),
              type: 'short',
              duration: '0:30',
              engagement: 93,
              isShort: true,
              avgViewDuration: '0:25'
            },
            {
              id: 'mock_4',
              title: 'Single moms, when did you realize you had to cut out?',
              views: 1076414,
              likes: Math.floor(1076414 * 0.02),
              comments: Math.floor(1076414 * 0.001),
              url: 'https://www.youtube.com/shorts/abc456',
              preview: 'https://img.youtube.com/vi/abc456/maxresdefault.jpg',
              thumbnail: 'https://img.youtube.com/vi/abc456/mqdefault.jpg',
              posted_date: new Date('2025-05-12').toISOString(),
              type: 'short',
              duration: '0:30',
              engagement: 87,
              isShort: true,
              avgViewDuration: '0:25'
            },
            {
              id: 'mock_5',
              title: 'What secret would you never tell your family but would?',
              views: 976414,
              likes: Math.floor(976414 * 0.02),
              comments: Math.floor(976414 * 0.001),
              url: 'https://www.youtube.com/shorts/def789',
              preview: 'https://img.youtube.com/vi/def789/maxresdefault.jpg',
              thumbnail: 'https://img.youtube.com/vi/def789/mqdefault.jpg',
              posted_date: new Date('2025-05-11').toISOString(),
              type: 'short',
              duration: '0:30',
              engagement: 89,
              isShort: true,
              avgViewDuration: '0:25'
            }
          ];
          console.log(`ğŸ¬ Created ${topVideos.length} mock videos for display`);
        }

        console.log(`ğŸ¬ Raw top videos sample:`, topVideos.slice(0, 2).map(v => ({
          video_id: v.id,
          title: v.title,
          views: v.views,
          url: v.url
        })));
      } catch (topVideosError) {
        console.log(`âš ï¸ Top videos failed:`, topVideosError.message);
        topVideos = [];
      }
    }

    // Calculate performance metrics
    const avgDailyViews = chartData.length > 0 ? Math.floor(totalViews / chartData.length) : 0;
    const highestDay = chartData.length > 0 ? Math.max(...chartData.map(d => d.views)) : 0;
    const lowestDay = chartData.length > 0 ? Math.min(...chartData.map(d => d.views)) : 0;

    // Top videos are already transformed from BigQuery or InfluxDB
    const transformedTopVideos = topVideos;

    // Get latest content (first video from top videos or from PostgreSQL)
    let latestContent = null;
    if (transformedTopVideos.length > 0) {
      // Use the first top video as latest content with complete data structure
      latestContent = {
        ...transformedTopVideos[0],
        engagement: Math.floor(Math.random() * 15 + 5) // Placeholder engagement percentage
      };
    } else {
      // Fallback: Get latest content from PostgreSQL
      try {

        const latestQuery = `
          SELECT url, upload_date
          FROM video
          WHERE writer_id = $1
          ORDER BY upload_date DESC
          LIMIT 1
        `;
        const latestResult = await pool.query(latestQuery, [writerId]);
        if (latestResult.rows.length > 0) {
          const latest = latestResult.rows[0];
          latestContent = {
            id: 'pg_' + Date.now(),
            title: 'Latest Video',
            views: 0, // No view data from PostgreSQL
            type: latest.url && latest.url.includes('/shorts/') ? 'short' : 'video',
            duration: latest.url && latest.url.includes('/shorts/') ? '0:30' : '5:00',
            posted_date: latest.upload_date,
            url: latest.url,
            engagement: 0
          };
          console.log('ğŸ“º Latest content from PostgreSQL:', latestContent.title);
        }
      } catch (pgError) {
        console.log('âš ï¸ Could not get latest content from PostgreSQL:', pgError.message);
      }
    }

    // If still no latest content, create mock latest content
    if (!latestContent) {
      console.log('ğŸ“º Creating mock latest content');
      latestContent = {
        id: 'mock_latest',
        title: 'Has your teen ever stolen your business?',
        views: 3500,
        likes: Math.floor(3500 * 0.02),
        comments: Math.floor(3500 * 0.001),
        type: 'short',
        duration: '0:30',
        posted_date: new Date('2025-05-23').toISOString(),
        url: 'https://www.youtube.com/shorts/latest123',
        preview: 'https://img.youtube.com/vi/latest123/maxresdefault.jpg',
        thumbnail: 'https://img.youtube.com/vi/latest123/mqdefault.jpg',
        engagement: 95,
        isShort: true,
        avgViewDuration: '0:25'
      };
    }

    console.log(`ğŸ” ANALYTICS DATA SUMMARY:`);
    console.log(`   - Data source: ${dataSource}`);
    console.log(`   - Total views: ${totalViews}`);
    console.log(`   - Chart data points: ${chartData.length}`);
    console.log(`   - Top videos: ${transformedTopVideos.length}`);
    console.log(`   - Latest content: ${latestContent ? 'YES' : 'NO'}`);
    console.log(`   - Has data issues: ${hasDataIssues}`);

    // Calculate trend based on recent data
    let trend = 'stable';
    if (chartData.length >= 7) {
      const recentWeek = chartData.slice(-7);
      const previousWeek = chartData.slice(-14, -7);
      if (recentWeek.length > 0 && previousWeek.length > 0) {
        const recentAvg = recentWeek.reduce((sum, day) => sum + day.views, 0) / recentWeek.length;
        const previousAvg = previousWeek.reduce((sum, day) => sum + day.views, 0) / previousWeek.length;
        trend = recentAvg > previousAvg ? 'up' : recentAvg < previousAvg ? 'down' : 'stable';
      }
    }

    // Get total submissions count for this writer from PostgreSQL (all YouTube videos)
    let totalSubmissionsCount = transformedTopVideos.length; // fallback
    try {
      const totalSubmissionsQuery = `
        SELECT COUNT(*) as total_count
        FROM video
        WHERE writer_id = $1
          AND (url LIKE '%youtube.com%' OR url LIKE '%youtu.be%')
          AND url IS NOT NULL
      `;

      const totalSubmissionsResult = await pool.query(totalSubmissionsQuery, [writerId]);

      if (totalSubmissionsResult.rows.length > 0) {
        totalSubmissionsCount = parseInt(totalSubmissionsResult.rows[0].total_count) || transformedTopVideos.length;
        console.log(`ğŸ“Š Total YouTube submissions for writer ${writerId}: ${totalSubmissionsCount} (all time from PostgreSQL)`);
      }
    } catch (totalSubmissionsError) {
      console.error('âŒ Error getting total submissions count from PostgreSQL:', totalSubmissionsError);
    }

    const analyticsData = {
      totalViews: totalViews,
      totalLikes: totalLikes || 0,
      totalComments: totalComments || 0,
      avgDailyViews: avgDailyViews,
      hasDataIssues: hasDataIssues,
      dateRange: range,
      chartData: chartData,
      topVideos: transformedTopVideos,
      latestContent: latestContent,
      totalSubmissions: totalSubmissionsCount,

      // Enhanced performance summary
      summary: {
        highestDay: highestDay,
        lowestDay: lowestDay,
        trend: trend,
        progressToTarget: Math.min((totalViews / 100000000) * 100, 100), // Progress to 100M
        totalVideos: transformedTopVideos.length,
        avgViewsPerVideo: transformedTopVideos.length > 0 ? Math.floor(totalViews / transformedTopVideos.length) : 0,
        topVideoViews: transformedTopVideos.length > 0 ? transformedTopVideos[0].views : 0,
        engagementRate: transformedTopVideos.length > 0 ?
          Math.floor(transformedTopVideos.reduce((sum, v) => sum + v.engagement, 0) / transformedTopVideos.length) : 0
      },

      // Performance metrics for dashboard cards
      performance: {
        viewsGrowth: trend === 'up' ? '+12.5%' : trend === 'down' ? '-3.2%' : '0%',
        engagementRate: '8.7%',
        avgWatchTime: '2:45',
        subscriberGrowth: '+5.2%'
      },

      metadata: {
        lastUpdated: new Date().toISOString(),
        dataQuality: hasDataIssues ? 'partial' : 'complete',
        source: dataSource,
        writerId: writerId,
        userId: userId,
        rangeProcessed: influxRange
      }
    };

    console.log('ğŸš€ SENDING HYBRID DATA TO FRONTEND:', JSON.stringify({
      totalViews: analyticsData.totalViews,
      totalLikes: analyticsData.totalLikes,
      totalComments: analyticsData.totalComments,
      avgDailyViews: analyticsData.avgDailyViews,
      chartDataLength: analyticsData.chartData.length,
      topVideosCount: analyticsData.topVideos?.length || 0,
      latestContent: analyticsData.latestContent ? 'YES' : 'NO',
      hasDataIssues: analyticsData.hasDataIssues,
      dataSource: dataSource
    }));

    console.log('ğŸ¬ TOP VIDEOS DATA:', analyticsData.topVideos?.slice(0, 3).map(v => ({
      id: v.id,
      title: v.title?.substring(0, 30) + '...',
      views: v.views
    })));

    res.json(analyticsData);
  } catch (error) {
    console.error('âŒ Channel analytics error:', error);
    res.status(500).json({ message: 'Server error' });
  }
});

// Get writer views data from BigQuery (for WriterAnalytics component)
router.get('/writer/views', authenticateToken, async (req, res) => {
  try {
    const { writer_id, startDate, endDate } = req.query;

    console.log('ğŸ“Š BigQuery views endpoint called:', { writer_id, startDate, endDate });

    if (!writer_id || !startDate || !endDate) {
      return res.status(400).json({
        error: 'Missing required parameters: writer_id, startDate, endDate'
      });
    }

    try {
      // Get BigQuery views data
      const bigQueryRows = await getBigQueryViews(writer_id, startDate, endDate, influxService);
      console.log(`ğŸ“Š BigQuery returned ${bigQueryRows.length} rows for writer ${writer_id}`);

      // Transform data to match WriterAnalytics component format
      // FIX: Convert BigQuery date to string to prevent timezone shifts
      const transformedData = bigQueryRows.map(row => ({
        time: { value: row.time.value instanceof Date ?
          row.time.value.toISOString().split('T')[0] :
          row.time.value },
        views: parseInt(row.views)
      }));

      console.log(`âœ… Sending ${transformedData.length} BigQuery data points to WriterAnalytics`);
      res.json(transformedData);

    } catch (bigQueryError) {
      console.error('âŒ BigQuery error in writer/views endpoint:', bigQueryError);

      // Fallback to InfluxDB if BigQuery fails
      if (influxService) {
        console.log('ğŸ”„ Falling back to InfluxDB for writer/views');

        // Convert date range to InfluxDB format
        const start = new Date(startDate);
        const end = new Date(endDate);
        const daysDiff = Math.ceil((end - start) / (1000 * 60 * 60 * 24));
        const influxRange = `${daysDiff}d`;

        const dailyAnalytics = await influxService.getDashboardAnalytics(influxRange, writer_id);

        // Transform InfluxDB data to match BigQuery format with UTC to EST conversion
        const fallbackData = dailyAnalytics
          .filter(day => {
            // Convert InfluxDB UTC time to EST for date comparison
            const utcDate = new Date(day.date);
            const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
            const dayDate = estDate.toISOString().split('T')[0];
            return dayDate >= startDate && dayDate <= endDate;
          })
          .map(day => {
            // Convert InfluxDB UTC time to EST
            const utcDate = new Date(day.date);
            const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
            return {
              time: { value: estDate.toISOString().split('T')[0] },
              views: day.views
            };
          });

        console.log(`âœ… Sending ${fallbackData.length} InfluxDB fallback data points to WriterAnalytics`);
        res.json(fallbackData);
      } else {
        res.status(500).json({ error: 'Both BigQuery and InfluxDB failed' });
      }
    }

  } catch (error) {
    console.error('âŒ Writer views endpoint error:', error);
    res.status(500).json({ error: 'Server error' });
  }
});

// Writer videos endpoint for Content page - shows ALL videos from statistics_youtube_api
router.get('/writer/videos', async (req, res) => {
  try {
    const { writer_id, range = '28', page = '1', limit = '20', type = 'all' } = req.query;

    if (!writer_id) {
      console.log(`âŒ Writer Videos API: Missing writer_id`);
      return res.status(400).json({ error: 'missing writer_id' });
    }

    console.log(`ğŸ¬ Writer Videos API: Getting ALL videos for writer ${writer_id}, range: ${range}, page: ${page}, type: ${type}`);

    // Use the same function but modify it to show ALL videos
    const result = await getPostgresContentVideosWithBigQueryNames(writer_id, range, page, limit, type);

    console.log(`âœ… Writer Videos: Found ${result.videos.length} videos for writer ${writer_id}`);
    console.log(`ğŸ“Š Pagination: Page ${result.pagination.currentPage}/${result.pagination.totalPages}, Total: ${result.pagination.totalVideos}`);

    // Debug: Log video types being returned
    if (result.videos.length > 0) {
      const typeBreakdown = result.videos.reduce((acc, video) => {
        acc[video.type || 'unknown'] = (acc[video.type || 'unknown'] || 0) + 1;
        return acc;
      }, {});
      console.log(`ğŸ” API Response type breakdown for type='${type}':`, typeBreakdown);
      console.log(`ğŸ” Sample videos being returned:`, result.videos.slice(0, 3).map(v => ({
        title: v.title?.substring(0, 50),
        type: v.type,
        isShort: v.isShort,
        url: v.url?.substring(0, 50)
      })));
    }

    // Return the data in the format expected by Content.jsx
    res.json({
      videos: result.videos,
      pagination: result.pagination,
      typeCounts: {
        all: result.pagination.totalVideos,
        short: result.videos.filter(v => v.type === 'short').length,
        video: result.videos.filter(v => v.type === 'video').length
      }
    });

  } catch (error) {
    console.error('âŒ Writer Videos API error:', error);
    res.status(500).json({ error: 'Error getting writer videos', message: error.message });
  }
});

// Get content/videos data for Content page
router.get('/content', authenticateToken, async (req, res) => {
  try {
    const { range = '30d', type = 'all', limit = 10, sort = 'views' } = req.query;
    const userId = req.user.id;

    console.log('ğŸ¬ Getting content data for user ID:', userId, 'Range:', range, 'Type:', type);

    // Get writer information from PostgreSQL
    let writerId = null;
    try {

      const writerQuery = `
        SELECT w.id as writer_id
        FROM writer w
        WHERE w.login_id = $1
      `;
      const writerResult = await pool.query(writerQuery, [userId]);
      if (writerResult.rows.length > 0) {
        writerId = writerResult.rows[0].writer_id;
        console.log('âœ… Found writer ID:', writerId, 'for content');
      }
    } catch (dbError) {
      console.error('âŒ Error getting writer ID for content:', dbError);
    }

    if (influxService) {
      try {
        // Convert frontend range to InfluxDB range for content
        const convertContentRange = (range) => {
          switch (range) {
            case 'last7days': return '7d';
            case 'last28days': return '28d';
            case 'last30days': return '30d';
            case 'last90days': return '90d';
            case 'last365days': return '365d';
            case 'lifetime': return '3y'; // Use 3 years for lifetime
            case '2025': return '150d';
            case '2024': return '365d';
            case 'may': return '31d';
            case 'april': return '30d';
            case 'march': return '31d';
            default: return '30d';
          }
        };

        const influxRange = convertContentRange(range);
        console.log(`ğŸ¬ Converting range '${range}' to InfluxDB format '${influxRange}'`);

        // Get real content data from InfluxDB filtered by writer
        const contentData = await influxService.getWriterSubmissions(writerId, influxRange);

        // Transform data for Content page format
        const transformedContent = contentData
          .map((video, index) => {
            // Format the posted date properly
            let formattedDate = 'Unknown Date';
            if (video.submittedOn) {
              try {
                const date = new Date(video.submittedOn);
                formattedDate = date.toLocaleDateString('en-US', {
                  year: 'numeric',
                  month: 'short',
                  day: 'numeric'
                });
              } catch (dateError) {
                console.error('âŒ Error formatting date:', dateError);
              }
            }

            return {
              id: video.id || index + 1,
              title: video.title,
              thumbnail: `https://img.youtube.com/vi/${video.video_id}/mqdefault.jpg`,
              views: video.views,
              publishDate: formattedDate,
              posted_date: video.submittedOn,
              status: 'Published',
              type: video.url && video.url.includes('/shorts/') ? 'short' : 'video',
              url: video.url,
              engagement: Math.floor(Math.random() * 20) + 80, // Placeholder engagement
              duration: video.url && video.url.includes('/shorts/') ? '0:30' : '5:00',
              writer_name: video.writer_name,
              account_name: video.account_name || 'Unknown Account', // Include account_name from InfluxDB
              video_id: video.video_id,
              timestamp: new Date(video.submittedOn).getTime()
            };
          })
          .filter(item => {
            if (type === 'short') return item.type === 'short';
            if (type === 'video') return item.type === 'video';
            if (type === 'full_to_short') return item.type === 'full_to_short';
            return true; // 'all' or any other value
          })
          .sort((a, b) => {
            if (sort === 'latest') {
              return b.timestamp - a.timestamp; // Sort by date descending (newest first)
            }
            return b.views - a.views; // Default: sort by views descending
          })
          .slice(0, parseInt(limit));

        console.log('ğŸ¬ Real content data sent:', {
          count: transformedContent.length,
          range,
          writerId
        });

        res.json({
          success: true,
          data: transformedContent,
          metadata: {
            total: transformedContent.length,
            range: range,
            type: type,
            sort: sort,
            limit: limit,
            writerId: writerId,
            source: 'InfluxDB - Real Data'
          }
        });
        return;
      } catch (influxError) {
        console.error('âŒ InfluxDB error in content, falling back to dummy data:', influxError);
      }
    }

    // Fallback to dummy data if InfluxDB fails
    const dummyContent = [
      {
        id: 1,
        title: 'Sample Video Title',
        thumbnail: 'https://via.placeholder.com/320x180',
        views: 150000,
        publishDate: new Date().toLocaleDateString(),
        status: 'Published',
        type: 'Short',
        engagement: 85,
        duration: '0:30',
        account_name: 'Sample Account', // Include account_name in dummy data
        writer_name: 'Sample Writer'
      }
    ];

    res.json({
      success: true,
      data: dummyContent,
      metadata: {
        total: dummyContent.length,
        range: range,
        type: type,
        source: 'Dummy Data - InfluxDB Unavailable'
      }
    });

  } catch (error) {
    console.error('âŒ Content endpoint error:', error);
    res.status(500).json({ message: 'Server error' });
  }
});

// Debug endpoint to check video data for writer 121 (mylogumbs) - no auth required
router.get('/debug-writer-121-videos', async (req, res) => {
  try {
    const writerId = 121;
    console.log(`ğŸ” DEBUG: Checking video data for writer ${writerId} (mylogumbs)`);

    // Check writer exists
    const writerQuery = `SELECT id, name FROM writer WHERE id = $1`;
    const { rows: writerRows } = await pool.query(writerQuery, [writerId]);
    console.log(`ğŸ‘¤ Writer info:`, writerRows[0] || 'Not found');

    // Check videos in video table with YouTube URLs
    const videoTableQuery = `
      SELECT id, script_title, url, video_cat, writer_id
      FROM video
      WHERE writer_id = $1
        AND (url LIKE '%youtube.com%' OR url LIKE '%youtu.be%')
        AND url IS NOT NULL
      ORDER BY id DESC
    `;
    const { rows: videoTableRows } = await pool.query(videoTableQuery, [writerId]);
    console.log(`ğŸ“Š YouTube videos in video table: ${videoTableRows.length}`);

    // Get total count using the same query as analytics
    const totalCountQuery = `
      SELECT COUNT(*) as total_count
      FROM video
      WHERE writer_id = $1
        AND (url LIKE '%youtube.com%' OR url LIKE '%youtu.be%')
        AND url IS NOT NULL
    `;
    const { rows: countRows } = await pool.query(totalCountQuery, [writerId]);
    const totalCount = countRows[0]?.total_count || 0;
    console.log(`ğŸ“Š Total count from analytics query: ${totalCount}`);

    // Check videos in statistics_youtube_api table
    const statsTableQuery = `
      SELECT video_id, duration, views_total, posted_date
      FROM statistics_youtube_api
      WHERE video_id IN (
        SELECT CAST(id AS VARCHAR) FROM video WHERE writer_id = $1
      )
      ORDER BY video_id DESC
    `;
    const { rows: statsTableRows } = await pool.query(statsTableQuery, [writerId]);
    console.log(`ğŸ“Š Videos in statistics_youtube_api table: ${statsTableRows.length}`);

    // Check which videos are missing from statistics_youtube_api
    const videoIds = videoTableRows.map(v => v.id.toString());
    const statsVideoIds = new Set(statsTableRows.map(s => s.video_id));
    const missingFromStats = videoTableRows.filter(v => !statsVideoIds.has(v.id.toString()));

    // Check INNER JOIN result (what the current query returns)
    const innerJoinQuery = `
      SELECT v.id, v.script_title, v.url, s.duration, s.views_total
      FROM video v
      INNER JOIN statistics_youtube_api s ON CAST(v.id AS VARCHAR) = s.video_id
      WHERE v.writer_id = $1
        AND v.url LIKE '%youtube.com%'
      ORDER BY v.id DESC
    `;
    const { rows: innerJoinRows } = await pool.query(innerJoinQuery, [writerId]);

    res.json({
      success: true,
      writerId: writerId,
      videoTable: {
        count: videoTableRows.length,
        sample: videoTableRows.slice(0, 5).map(v => ({
          id: v.id,
          title: v.script_title,
          url: v.url?.substring(0, 50) + '...',
          category: v.video_cat
        }))
      },
      statisticsTable: {
        count: statsTableRows.length,
        sample: statsTableRows.slice(0, 5).map(s => ({
          video_id: s.video_id,
          duration: s.duration,
          views: s.views_total,
          posted_date: s.posted_date
        }))
      },
      missingFromStats: {
        count: missingFromStats.length,
        videos: missingFromStats.slice(0, 10).map(v => ({
          id: v.id,
          title: v.script_title,
          url: v.url?.substring(0, 50) + '...',
          category: v.video_cat
        }))
      },
      innerJoinResult: {
        count: innerJoinRows.length,
        sample: innerJoinRows.slice(0, 5).map(v => ({
          id: v.id,
          title: v.script_title,
          duration: v.duration,
          views: v.views_total
        }))
      }
    });

  } catch (error) {
    console.error('âŒ Debug writer 121 videos error:', error);
    res.status(500).json({ error: error.message });
  }
});

// Simple test endpoint to verify frontend connectivity
router.get('/test-simple', authenticateToken, async (req, res) => {
  console.log('ğŸ§ª Simple test endpoint called');
  res.json({
    success: true,
    message: 'Frontend can reach backend!',
    totalViews: 139616232175,
    chartData: [
      { date: '2024-01-01', views: 1000000 },
      { date: '2024-01-02', views: 1200000 }
    ],
    timestamp: new Date().toISOString()
  });
});

// Test endpoint to see what frontend should receive
router.get('/test-frontend', authenticateToken, async (req, res) => {
  try {
    const writerId = req.user.writerId || req.user.userId;
    const range = '30d';

    console.log('ğŸ§ª Test endpoint called for writer:', writerId);

    if (influxService) {
      const [totalViews, dailyAnalytics] = await Promise.all([
        influxService.getTotalViews(range, writerId),
        influxService.getDashboardAnalytics(range, writerId)
      ]);

      const chartData = dailyAnalytics.map(day => {
        // Convert InfluxDB UTC time to EST
        const utcDate = new Date(day.date);
        const estDate = new Date(utcDate.getTime() - (5 * 60 * 60 * 1000)); // UTC-5 for EST
        return {
          date: estDate.toISOString().split('T')[0],
          views: day.views,
          timestamp: estDate.getTime()
        };
      });

      const testData = {
        totalViews: totalViews,
        chartData: chartData,
        avgDailyViews: chartData.length > 0 ? Math.floor(totalViews / chartData.length) : 0,
        summary: {
          highestDay: chartData.length > 0 ? Math.max(...chartData.map(d => d.views)) : 0,
          lowestDay: chartData.length > 0 ? Math.min(...chartData.map(d => d.views)) : 0,
          progressToTarget: Math.min((totalViews / 100000000) * 100, 100)
        }
      };

      console.log('ğŸ§ª Test data prepared:', {
        totalViews: testData.totalViews,
        chartDataPoints: testData.chartData.length,
        avgDaily: testData.avgDailyViews
      });

      res.json({
        success: true,
        data: testData,
        debug: {
          writerId: writerId,
          range: range,
          rawTotalViews: totalViews,
          rawChartPoints: dailyAnalytics.length
        }
      });
    } else {
      res.json({
        success: false,
        error: 'InfluxDB service not available'
      });
    }
  } catch (error) {
    console.error('ğŸ§ª Test endpoint error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// Debug endpoint to explore InfluxDB data structure
router.get('/debug-influx', authenticateToken, async (req, res) => {
  try {
    const writerId = req.user.writerId || req.user.userId;
    console.log('ğŸ” Debug: Exploring InfluxDB data for writer:', writerId);

    if (!influxService) {
      return res.json({
        error: 'InfluxDB service not initialized',
        writerId: writerId
      });
    }

    const results = {
      writerId: writerId,
      measurements: [],
      sampleData: [],
      writerSpecificData: [],
      fieldAnalysis: {}
    };

    // 1. Get all measurements
    try {
      const measurementsQuery = `
        import "influxdata/influxdb/schema"
        schema.measurements(bucket: "youtube_api")
      `;

      await influxService.queryApi.queryRows(measurementsQuery, {
        next(row, tableMeta) {
          const o = tableMeta.toObject(row);
          results.measurements.push(o._value);
        },
        error(error) {
          console.error('Error getting measurements:', error);
          results.measurements.push('Error: ' + error.message);
        },
        complete() {
          console.log('ğŸ“Š Available measurements:', results.measurements);
        }
      });
    } catch (error) {
      results.measurements = ['Error getting measurements: ' + error.message];
    }

    // 2. Get sample data from the last 30 days
    try {
      const sampleQuery = `
        from(bucket: "youtube_api")
          |> range(start: -30d)
          |> limit(n: 5)
      `;

      await influxService.queryApi.queryRows(sampleQuery, {
        next(row, tableMeta) {
          const o = tableMeta.toObject(row);
          results.sampleData.push(o);
        },
        error(error) {
          console.error('Error getting sample data:', error);
          results.sampleData.push('Error: ' + error.message);
        },
        complete() {
          console.log('ğŸ“‹ Sample data count:', results.sampleData.length);
        }
      });
    } catch (error) {
      results.sampleData = ['Error getting sample data: ' + error.message];
    }

    // 3. Try to get writer-specific data
    if (writerId) {
      try {
        const writerQuery = `
          from(bucket: "youtube_api")
            |> range(start: -30d)
            |> filter(fn: (r) => r.writer_id == "${writerId}" or r.writer_id == ${writerId})
            |> limit(n: 5)
        `;

        await influxService.queryApi.queryRows(writerQuery, {
          next(row, tableMeta) {
            const o = tableMeta.toObject(row);
            results.writerSpecificData.push(o);
          },
          error(error) {
            console.error('Error getting writer-specific data:', error);
            results.writerSpecificData.push('Error: ' + error.message);
          },
          complete() {
            console.log('ğŸ‘¤ Writer-specific data count:', results.writerSpecificData.length);
          }
        });
      } catch (error) {
        results.writerSpecificData = ['Error getting writer data: ' + error.message];
      }
    }

    // 4. Get field analysis for main measurements
    for (const measurement of results.measurements.slice(0, 3)) {
      if (typeof measurement === 'string' && !measurement.includes('Error')) {
        try {
          const fieldsQuery = `
            import "influxdata/influxdb/schema"
            schema.fieldKeys(bucket: "youtube_api", measurement: "${measurement}")
          `;

          const fields = [];
          await influxService.queryApi.queryRows(fieldsQuery, {
            next(row, tableMeta) {
              const o = tableMeta.toObject(row);
              fields.push(o._value);
            },
            error(error) {
              console.error(`Error getting fields for ${measurement}:`, error);
            },
            complete() {
              results.fieldAnalysis[measurement] = fields;
            }
          });
        } catch (error) {
          results.fieldAnalysis[measurement] = ['Error: ' + error.message];
        }
      }
    }

    res.json({
      success: true,
      data: results,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('âŒ Debug endpoint error:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      writerId: req.user.writerId || req.user.userId
    });
  }
});

// BigQuery-powered Content page endpoint
router.get('/videos', authenticateToken, async (req, res) => {
  console.log(`ğŸš€ BIGQUERY CONTENT ENDPOINT CALLED! Query params:`, req.query);
  console.log(`ğŸš€ BIGQUERY CONTENT ENDPOINT: Headers:`, req.headers.authorization ? 'Token present' : 'No token');

  try {
    const { writer_id, range = '28', page = '1', limit = '20', type = 'all' } = req.query;

    if (!writer_id) {
      console.log(`âŒ BigQuery Content API: Missing writer_id`);
      return res.status(400).json({ error: 'missing writer_id' });
    }

    console.log(`ğŸ¬ PostgreSQL Content API: Getting videos for writer ${writer_id}, range: ${range}, page: ${page}, type: ${type}`);
    console.log(`ğŸ” PostgreSQL Content API: Authenticated user:`, req.user);

    try {
      console.log(`ğŸ” PostgreSQL Content API: About to call getPostgresContentVideosWithBigQueryNames...`);
      // Use PostgreSQL data enhanced with BigQuery account names
      const result = await getPostgresContentVideosWithBigQueryNames(writer_id, range, page, limit, type);

      console.log(`âœ… BigQuery Content: Found ${result.videos.length} videos for writer ${writer_id}`);
      console.log(`ğŸ“Š Pagination: Page ${result.pagination.currentPage}/${result.pagination.totalPages}, Total: ${result.pagination.totalVideos}`);

      // Debug: Log sample video data to see account names
      if (result.videos.length > 0) {
        console.log(`ğŸ” Sample BigQuery video data:`, {
          title: result.videos[0].title,
          account_name: result.videos[0].account_name,
          writer_name: result.videos[0].writer_name,
          views: result.videos[0].views,
          url: result.videos[0].url
        });
      }

      // Return the data in the format expected by Content.jsx
      if (result.pagination) {
        // Return paginated response
        res.json({
          videos: result.videos,
          pagination: result.pagination,
          typeCounts: {
            all: result.pagination.totalVideos,
            short: result.videos.filter(v => v.type === 'short').length,
            video: result.videos.filter(v => v.type === 'video').length
          }
        });
      } else {
        // Return simple array for backward compatibility
        res.json(result.videos);
      }

    } catch (bigQueryError) {
      console.error('âŒ BigQuery error in content endpoint:', bigQueryError);
      console.error('âŒ BigQuery error stack:', bigQueryError.stack);
      console.error('âŒ BigQuery error message:', bigQueryError.message);

      // Fallback to mock data
      console.log('ğŸ”„ Using mock data fallback for content');
      const mockVideos = [
        {
          id: 1,
          url: "https://youtube.com/shorts/zQrDuHoNZCc",
          title: "Sample Short Video",
          writer_id: writer_id,
          writer_name: "Writer",
          account_name: "YouTube Channel",
          preview: "https://i.ytimg.com/vi/zQrDuHoNZCc/default.jpg",
          views: 216577,
          likes: 8462,
          comments: 52,
          posted_date: new Date().toISOString(),
          duration: "0:45",
          type: "short",
          status: "Published"
        },
        {
          id: 2,
          url: "https://youtube.com/watch?v=dQw4w9WgXcQ",
          title: "Sample Long Video",
          writer_id: writer_id,
          writer_name: "Writer",
          account_name: "YouTube Channel",
          preview: "https://i.ytimg.com/vi/dQw4w9WgXcQ/maxresdefault.jpg",
          views: 89000,
          likes: 3200,
          comments: 180,
          posted_date: new Date(Date.now() - 86400000).toISOString(),
          duration: "15:30",
          type: "video",
          status: "Published"
        },
        {
          id: 3,
          url: "https://youtube.com/watch?v=sample3",
          title: "Sample Full to Short Video",
          writer_id: writer_id,
          writer_name: "Writer",
          account_name: "YouTube Channel",
          preview: "https://i.ytimg.com/vi/sample3/maxresdefault.jpg",
          views: 156000,
          likes: 7800,
          comments: 420,
          posted_date: new Date(Date.now() - 86400000 * 2).toISOString(),
          duration: "1:15",
          type: "full_to_short",
          status: "Published"
        }
      ];

      res.json({
        videos: mockVideos,
        pagination: {
          currentPage: 1,
          totalPages: 1,
          totalVideos: 2,
          videosPerPage: 20,
          hasNextPage: false,
          hasPrevPage: false
        },
        typeCounts: {
          all: 2,
          short: 1,
          video: 1
        }
      });
    }

  } catch (error) {
    console.error('âŒ BigQuery Content endpoint error:', error);
    console.error('âŒ Error stack:', error.stack);
    console.error('âŒ Error message:', error.message);
    res.status(500).json({ error: 'Failed to fetch content data', details: error.message });
  }
});

// Get top content for analytics page (PostgreSQL + BigQuery enhanced) - Works like Content page
router.get('/writer/top-content', authenticateToken, async (req, res) => {
  try {
    const { writer_id, range = '28', limit = '20', type = 'all', start_date, end_date } = req.query;

    if (!writer_id) {
      return res.status(400).json({ error: 'missing writer_id' });
    }

    console.log('ğŸ† Getting top content like Content page for writer:', writer_id, 'Range:', range, 'Type:', type, 'Limit:', limit, 'Custom dates:', { start_date, end_date });

    // Calculate date range - handle custom dates
    let startDate;
    let endDate;

    if (start_date && end_date) {
      // Use custom date range
      startDate = new Date(start_date);
      endDate = new Date(end_date);
      // For single day selection, set end date to end of day
      if (start_date === end_date) {
        endDate.setHours(23, 59, 59, 999);
      }
      console.log('ğŸ“… Using custom date range:', startDate.toISOString().split('T')[0], 'to', endDate.toISOString().split('T')[0]);
    } else if (range === 'lifetime') {
      // For lifetime, don't set date restrictions
      startDate = null;
      endDate = null;
      console.log('ğŸ“… Using lifetime range (no date restrictions)');
    } else {
      // Use predefined ranges
      endDate = new Date();
      switch (range) {
        case '7':
          startDate = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000);
          break;
        case '30':
          startDate = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);
          break;
        case '90':
          startDate = new Date(Date.now() - 90 * 24 * 60 * 60 * 1000);
          break;
        case '365':
          startDate = new Date(Date.now() - 365 * 24 * 60 * 60 * 1000);
          break;
        default:
          startDate = new Date(Date.now() - 28 * 24 * 60 * 60 * 1000);
      }
      console.log('ğŸ“… Using predefined date range:', startDate.toISOString().split('T')[0], 'to', endDate.toISOString().split('T')[0]);
    }

    // Use the same approach as Content page - get ALL videos first, then filter by type using BigQuery duration
    console.log('ğŸ¬ Using Content page approach: Get all videos from PostgreSQL, enhance with BigQuery, then filter by type');

    // Build date condition for PostgreSQL
    let dateCondition = '';
    let queryParams = [writer_id];
    if (startDate && endDate) {
      dateCondition = 'AND statistics_youtube_api.posted_date >= $2 AND statistics_youtube_api.posted_date <= $3';
      queryParams.push(startDate.toISOString(), endDate.toISOString());
    }

    // Query PostgreSQL for ALL videos (no type filtering yet) - like Content page
    const allVideosQuery = `
      SELECT
        video.id as video_id,
        video.script_title AS title,
        video.url,
        COALESCE(statistics_youtube_api.views_total, 0) AS views,
        COALESCE(statistics_youtube_api.likes_total, 0) AS likes,
        COALESCE(statistics_youtube_api.comments_total, 0) AS comments,
        statistics_youtube_api.posted_date,
        statistics_youtube_api.preview,
        statistics_youtube_api.duration
      FROM video
      LEFT JOIN statistics_youtube_api
          ON CAST(video.id AS VARCHAR) = statistics_youtube_api.video_id
      WHERE video.writer_id = $1
        AND (video.url LIKE '%youtube.com%' OR video.url LIKE '%youtu.be%')
        AND statistics_youtube_api.views_total IS NOT NULL
        ${dateCondition}
      ORDER BY statistics_youtube_api.views_total DESC
    `;

    console.log('ğŸ” PostgreSQL all videos query:', allVideosQuery);
    console.log('ğŸ“Š Query params:', queryParams);

    const result = await pool.query(allVideosQuery, queryParams);
    const allVideosRows = result.rows;

    console.log(`ğŸ¬ PostgreSQL returned ${allVideosRows.length} total videos for writer ${writer_id}`);

    // Step 1: Transform PostgreSQL data (like Content page)
    const postgresVideos = allVideosRows.map(row => {
      // Use actual duration from database (no fallbacks)
      const duration = row.duration;

      // Determine video type based on PostgreSQL duration first (fallback)
      let videoType = 'video'; // default
      let isShort = false;

      if (duration) {
        const parts = duration.split(':');
        if (parts.length >= 2) {
          const minutes = parseInt(parts[0]) || 0;
          const seconds = parseInt(parts[1]) || 0;
          const totalSeconds = minutes * 60 + seconds;

          if (totalSeconds < 183) { // Less than 3 minutes 3 seconds (183 seconds)
            videoType = 'short';
            isShort = true;
          }
        }
      }

      // Calculate dynamic engagement rate based on actual data
      const views = parseInt(row.views) || 0;
      const likes = parseInt(row.likes) || 0;
      const comments = parseInt(row.comments) || 0;

      let engagementRate = 0;
      if (views > 0) {
        engagementRate = ((likes + comments) / views) * 100;
        engagementRate = Math.min(100, Math.round(engagementRate * 10) / 10);
      }

      return {
        id: row.video_id,
        title: row.title || 'Untitled Video',
        url: row.url,
        views: views,
        likes: likes,
        comments: comments,
        type: videoType,
        isShort: isShort,
        posted_date: row.posted_date,
        thumbnail: row.preview || `https://img.youtube.com/vi/${extractVideoId(row.url)}/maxresdefault.jpg`,
        duration: duration,
        engagement: engagementRate
      };
    });

    // Step 2: Enhance with BigQuery duration data (like Content page)
    console.log('ğŸ” Step 2: Enhancing with BigQuery duration data...');

    // Extract video IDs for BigQuery lookup
    const videoIds = postgresVideos.map(video => extractVideoId(video.url)).filter(id => id);
    console.log(`ğŸ” Extracted ${videoIds.length} video IDs for BigQuery lookup`);

    let enhancedVideos = postgresVideos;
    let bigQueryDurations = new Map();

    if (videoIds.length > 0) {
      try {
        // Query BigQuery for duration data and account names from youtube_video_report_historical
        const bigQuerySql = `
          SELECT
            video_id,
            video_duration_seconds,
            channel_title
          FROM \`speedy-web-461014-g3.dbt_youtube_analytics.youtube_video_report_historical\`
          WHERE video_id IN UNNEST(@video_ids)
          GROUP BY video_id, video_duration_seconds, channel_title
        `;

        const bigQueryOptions = {
          query: bigQuerySql,
          params: { video_ids: videoIds }
        };

        const [bigQueryRows] = await bigquery.query(bigQueryOptions);
        console.log(`ğŸ“Š BigQuery returned ${bigQueryRows.length} duration records`);

        // Create duration map with channel_title as account name
        bigQueryRows.forEach(row => {
          if (row.video_id && row.video_duration_seconds !== null) {
            bigQueryDurations.set(row.video_id, {
              duration_seconds: parseFloat(row.video_duration_seconds),
              channel_title: row.channel_title,
              account_name: row.channel_title // Use channel_title as account_name
            });
          }
        });

        console.log(`ğŸ“Š BigQuery duration map created with ${bigQueryDurations.size} entries`);
      } catch (bigQueryError) {
        console.warn('âš ï¸ BigQuery duration lookup failed:', bigQueryError.message);
      }
    }

    // Step 3: Apply BigQuery duration data and re-determine video types
    enhancedVideos = postgresVideos.map(video => {
      const youtubeVideoId = extractVideoId(video.url);
      const bigQueryData = bigQueryDurations.get(youtubeVideoId);

      let finalType = video.type; // Start with PostgreSQL type
      let finalIsShort = video.isShort;

      // Use BigQuery duration if available (priority over PostgreSQL)
      if (bigQueryData && bigQueryData.duration_seconds > 0) {
        if (bigQueryData.duration_seconds < 183) {
          finalType = 'short';
          finalIsShort = true;
        } else {
          finalType = 'video';
          finalIsShort = false;
        }
        console.log(`âœ… Using BigQuery duration for video ${video.id}: ${bigQueryData.duration_seconds}s -> ${finalType}`);
      } else {
        console.log(`âš ï¸ No BigQuery duration data for video ${video.id}, using PostgreSQL type: ${finalType} (isShort: ${finalIsShort})`);
      }

      // Format duration properly from BigQuery seconds or PostgreSQL duration
      let formattedDuration = video.duration; // Default to PostgreSQL duration
      if (bigQueryData?.duration_seconds && bigQueryData.duration_seconds > 0) {
        const totalSeconds = Math.round(bigQueryData.duration_seconds);
        const minutes = Math.floor(totalSeconds / 60);
        const seconds = totalSeconds % 60;
        formattedDuration = `${minutes}:${seconds.toString().padStart(2, '0')}`;
      }

      return {
        ...video,
        type: finalType,
        isShort: finalIsShort,
        video_duration_seconds: bigQueryData?.duration_seconds || null,
        duration: formattedDuration, // Use properly formatted duration
        account_name: bigQueryData?.channel_title || 'Unknown Account', // Use channel_title from youtube_video_report_historical
        channel_title: bigQueryData?.channel_title,
        thumbnail: video.thumbnail // Keep original thumbnail
      };
    });

    // Step 4: Apply type filtering with balanced results for 'all'
    let filteredVideos = enhancedVideos;
    let topContent = [];

    console.log(`ğŸ” Filtering by type: '${type}' with limit: ${limit}`);

    if (type === 'shorts') {
      filteredVideos = enhancedVideos.filter(video => video.isShort);
      console.log(`ğŸ” Filtering for shorts: ${filteredVideos.length} videos where isShort=true`);

      // Debug: Log videos that are being classified as shorts
      console.log('ğŸ” DEBUG: Videos classified as shorts:');
      filteredVideos.forEach(video => {
        console.log(`  - ${video.title?.substring(0, 50)}... | Duration: ${video.duration} | Duration Seconds: ${video.video_duration_seconds} | Type: ${video.type} | isShort: ${video.isShort}`);
      });

      topContent = filteredVideos
        .sort((a, b) => b.views - a.views)
        .slice(0, parseInt(limit));
    } else if (type === 'content' || type === 'videos') {
      filteredVideos = enhancedVideos.filter(video => !video.isShort);
      console.log(`ğŸ” Filtering for videos: ${filteredVideos.length} videos where isShort=false`);

      // Debug: Log videos that are being classified as videos
      console.log('ğŸ” DEBUG: Videos classified as videos:');
      filteredVideos.forEach(video => {
        console.log(`  - ${video.title?.substring(0, 50)}... | Duration: ${video.duration} | Duration Seconds: ${video.video_duration_seconds} | Type: ${video.type} | isShort: ${video.isShort}`);
      });

      topContent = filteredVideos
        .sort((a, b) => b.views - a.views)
        .slice(0, parseInt(limit));
    } else if (type === 'all') {
      // For 'all', get balanced results: 10 shorts + 10 videos (or half/half based on limit)
      const halfLimit = Math.floor(parseInt(limit) / 2);

      const topShorts = enhancedVideos
        .filter(video => video.isShort)
        .sort((a, b) => b.views - a.views)
        .slice(0, halfLimit);

      const topVideos = enhancedVideos
        .filter(video => !video.isShort)
        .sort((a, b) => b.views - a.views)
        .slice(0, halfLimit);

      // Combine and sort by views
      topContent = [...topShorts, ...topVideos]
        .sort((a, b) => b.views - a.views);

      console.log(`ğŸ” Balanced results for 'all': ${topShorts.length} shorts + ${topVideos.length} videos = ${topContent.length} total`);
    }

    console.log('ğŸ† Top content found after filtering and limiting:', topContent.length, 'videos');
    console.log('ğŸ“Š Sample top content:', topContent[0]);

    // Step 6: Enhance with writer names
    let enhancedTopContent = topContent;
    try {
      console.log('ğŸ” Adding writer names to top content...');

      // Get writer name from PostgreSQL writer table
      const writerQuery = 'SELECT writer_name FROM writer WHERE id = $1';
      const writerResult = await pool.query(writerQuery, [writer_id]);
      const writerName = writerResult.rows[0]?.writer_name || 'Unknown Writer';

      // Add writer name to all videos
      enhancedTopContent = topContent.map(video => ({
        ...video,
        writer_name: writerName
      }));

      console.log('âœ… Enhanced top content with writer names');
      console.log('ğŸ“Š Sample enhanced content:', enhancedTopContent[0]);

    } catch (enhanceError) {
      console.warn('âš ï¸ Could not enhance with writer names:', enhanceError.message);
      enhancedTopContent = topContent;
    }

    // Debug: Log what we're actually sending to frontend
    console.log('ğŸ“¤ Sending top content response to frontend:');
    console.log('ğŸ“Š Number of videos:', enhancedTopContent.length);
    if (enhancedTopContent.length > 0) {
      console.log('ğŸ“Š Sample video data:', {
        title: enhancedTopContent[0].title,
        account_name: enhancedTopContent[0].account_name,
        writer_name: enhancedTopContent[0].writer_name,
        views: enhancedTopContent[0].views
      });
      console.log('ğŸ“Š All account names:', enhancedTopContent.map(v => ({
        title: v.title?.substring(0, 30) + '...',
        account_name: v.account_name
      })));
    }

    res.json({
      success: true,
      data: enhancedTopContent,
      metadata: {
        writer_id: writer_id,
        range: range,
        type: type,
        total_found: enhancedTopContent.length,
        source: 'PostgreSQL + BigQuery Enhanced'
      }
    });

  } catch (error) {
    console.error('âŒ Error getting top content:', error);
    res.status(500).json({
      error: 'Failed to get top content',
      details: error.message
    });
  }
});

// Get latest content for analytics page (PostgreSQL + BigQuery enhanced)
router.get('/writer/latest-content', authenticateToken, async (req, res) => {
  try {
    const { writer_id } = req.query;

    if (!writer_id) {
      return res.status(400).json({ error: 'missing writer_id' });
    }

    console.log('ğŸ“… Getting latest content with BigQuery enhancement for writer:', writer_id);

    // Get latest content from PostgreSQL
    const latestContentQuery = `
          SELECT
            video.id as video_id,
            video.script_title AS title,
            video.url,
            COALESCE(statistics_youtube_api.views_total, 0) AS views,
            COALESCE(statistics_youtube_api.likes_total, 0) AS likes,
            COALESCE(statistics_youtube_api.comments_total, 0) AS comments,
            COALESCE(statistics_youtube_api.posted_date, video.created) AS posted_date,
            statistics_youtube_api.preview,
            statistics_youtube_api.duration
          FROM video
          LEFT JOIN statistics_youtube_api
          ON CAST(video.id AS VARCHAR) = statistics_youtube_api.video_id
      WHERE video.writer_id = $1
        AND video.url LIKE '%youtube.com%'
        AND statistics_youtube_api.posted_date IS NOT NULL
      ORDER BY statistics_youtube_api.posted_date DESC
      LIMIT 1
    `;

    console.log('ğŸ” PostgreSQL latest content query:', latestContentQuery);
    const result = await pool.query(latestContentQuery, [writer_id]);
    const latestContentRows = result.rows;

    // Filter to only videos with duration data
    const videosWithDuration = latestContentRows.filter(row => row.duration);

    if (videosWithDuration.length === 0) {
      return res.json({
        success: true,
        data: null,
        message: 'No recent content found with duration data'
      });
    }

    const row = videosWithDuration[0];

    // Format the posted date properly
    let formattedDate = 'Unknown Date';
    if (row.posted_date) {
      try {
        const date = new Date(row.posted_date);
        formattedDate = date.toLocaleDateString('en-US', {
          year: 'numeric',
          month: 'short',
          day: 'numeric'
        });
      } catch (dateError) {
        console.error('âŒ Error formatting date:', dateError);
      }
    }

    // Use actual duration from database (no fallbacks)
    const duration = row.duration;

    // Determine video type based on duration (< 3 minutes = short, >= 3 minutes = video)
    let videoType = 'video'; // default
    let isShort = false;

    const parts = duration.split(':');
    if (parts.length >= 2) {
      const minutes = parseInt(parts[0]) || 0;
      const seconds = parseInt(parts[1]) || 0;
      const totalSeconds = minutes * 60 + seconds;

      if (totalSeconds < 183) { // Less than 3 minutes 3 seconds (183 seconds)
        videoType = 'short';
        isShort = true;
      }
    }

    // Calculate dynamic engagement rate based on actual data
    const views = parseInt(row.views) || 0;
    const likes = parseInt(row.likes) || 0;
    const comments = parseInt(row.comments) || 0;

    // Calculate engagement as (likes + comments) / views * 100
    let engagementRate = 0;
    if (views > 0) {
      engagementRate = ((likes + comments) / views) * 100;
      // Cap at 100% and round to 1 decimal place
      engagementRate = Math.min(100, Math.round(engagementRate * 10) / 10);
    }

    // Calculate "Stayed to Watch" from BigQuery retention data if available
    let stayedToWatch = null;
    try {
      const videoId = extractVideoId(row.url);
      if (videoId) {
        const retentionQuery = `
          SELECT AVG(audience_watch_ratio) * 100 AS percent_stayed
          FROM \`speedy-web-461014-g3.dbt_youtube_analytics.audience_retention_historical\`
          WHERE video_id = @video_id
            AND date = (SELECT MAX(date) FROM \`speedy-web-461014-g3.dbt_youtube_analytics.audience_retention_historical\` WHERE video_id = @video_id)
            AND elapsed_video_time_ratio BETWEEN 0.9 AND 1.0
        `;

        const retentionOptions = {
          query: retentionQuery,
          params: { video_id: videoId }
        };

        const [retentionRows] = await bigquery.query(retentionOptions);
        if (retentionRows.length > 0 && retentionRows[0].percent_stayed !== null) {
          stayedToWatch = parseFloat(retentionRows[0].percent_stayed);
          console.log(`ğŸ“Š Stayed to Watch for ${videoId}:`, stayedToWatch.toFixed(1) + '%');
        }
      }
    } catch (retentionError) {
      console.warn('âš ï¸ Could not fetch retention data for latest content:', retentionError.message);
    }

    const latestContent = {
      id: row.video_id,
      title: row.title || 'Untitled Video',
      url: row.url,
      views: views,
      likes: likes,
      comments: comments,
      type: videoType,
      isShort: isShort,
      posted_date: row.posted_date,
      publishDate: formattedDate,
      thumbnail: row.preview || `https://img.youtube.com/vi/${extractVideoId(row.url)}/maxresdefault.jpg`,
      duration: duration,
      engagement: engagementRate,
      stayedToWatch: stayedToWatch
    };

    console.log('ğŸ“… Latest content found from PostgreSQL:', latestContent.title);

    // Enhance with BigQuery data for account names and writer names (using Content page approach)
    let enhancedLatestContent = latestContent;
    try {
      console.log('ğŸ” Enhancing latest content with BigQuery data using Content page approach...');

      // Get writer name from PostgreSQL writer table
      const writerQuery = 'SELECT writer_name FROM writer WHERE id = $1';
      const writerResult = await pool.query(writerQuery, [writer_id]);
      const writerName = writerResult.rows[0]?.writer_name || 'Unknown Writer';

      // Extract video_id from URL for BigQuery lookup
      const videoId = extractVideoId(latestContent.url);
      console.log('ğŸ” Extracted video ID for BigQuery lookup:', videoId, 'from URL:', latestContent.url);

      if (videoId) {
        // Query BigQuery for enhanced data (using youtube_video_report_historical like top content)
        const bigQuerySql = `
          SELECT
            video_id,
            channel_title
          FROM \`speedy-web-461014-g3.dbt_youtube_analytics.youtube_video_report_historical\`
          WHERE video_id = @video_id
          GROUP BY video_id, channel_title
          LIMIT 1
        `;

        const bigQueryOptions = {
          query: bigQuerySql,
          params: { video_id: videoId }
        };

        console.log('ğŸ” BigQuery query for latest content:', bigQuerySql);
        console.log('ğŸ” BigQuery params:', bigQueryOptions.params);

        const [bigQueryRows] = await bigquery.query(bigQueryOptions);
        console.log('ğŸ“Š BigQuery returned', bigQueryRows.length, 'rows for latest content');

        // Use direct BigQuery approach (like top content)
        if (bigQueryRows.length > 0) {
          const bigQueryData = bigQueryRows[0];
          console.log('âœ… BigQuery data found for latest content:', {
            video_id: bigQueryData.video_id,
            channel_title: bigQueryData.channel_title
          });

          // Use channel_title as account name (like top content)
          const enhancedAccountName = bigQueryData.channel_title || 'Not Available';

          // Enhance the latest content with BigQuery data
          enhancedLatestContent = {
            ...latestContent,
            account_name: enhancedAccountName,
            writer_name: writerName,
            channelTitle: bigQueryData.channel_title,
            thumbnail: latestContent.thumbnail // Keep original thumbnail
          };
          console.log('âœ… Enhanced latest content account_name:', enhancedLatestContent.account_name);
        } else {
          console.log('âŒ No BigQuery data found for video ID:', videoId);
          // No BigQuery data found, add writer name only
          enhancedLatestContent = {
            ...latestContent,
            account_name: 'Not Available',
            writer_name: writerName
          };
        }
      } else {
        // Could not extract video ID, add writer name only
        enhancedLatestContent = {
          ...latestContent,
          account_name: 'Not Available',
          writer_name: writerName
        };
      }

      console.log('âœ… Enhanced latest content with BigQuery data');
      console.log('ğŸ“Š Enhanced latest content:', enhancedLatestContent.title, 'Account:', enhancedLatestContent.account_name);

    } catch (enhanceError) {
      console.warn('âš ï¸ Could not enhance latest content with BigQuery data:', enhanceError.message);
      // Continue with PostgreSQL data only, but add writer name
      try {
        const writerQuery = 'SELECT writer_name FROM writer WHERE id = $1';
        const writerResult = await pool.query(writerQuery, [writer_id]);
        const writerName = writerResult.rows[0]?.writer_name || 'Unknown Writer';

        enhancedLatestContent = {
          ...latestContent,
          account_name: 'Not Available',
          writer_name: writerName
        };
      } catch (writerError) {
        console.warn('âš ï¸ Could not get writer name:', writerError.message);
      }
    }

    // Debug: Log what we're actually sending to frontend
    console.log('ğŸ“¤ Sending latest content response to frontend:');
    console.log('ğŸ“Š Latest content data:', {
      title: enhancedLatestContent.title,
      account_name: enhancedLatestContent.account_name,
      writer_name: enhancedLatestContent.writer_name,
      views: enhancedLatestContent.views
    });

    res.json({
      success: true,
      data: enhancedLatestContent,
      metadata: {
        writer_id: writer_id,
        source: 'PostgreSQL + BigQuery Enhanced'
      }
    });

  } catch (error) {
    console.error('âŒ Error getting latest content:', error);
    res.status(500).json({
      error: 'Failed to get latest content',
      details: error.message
    });
  }
});

// Test endpoint for new BigQuery function (no auth for testing)
router.get('/test-new-bigquery', async (req, res) => {
  try {
    const { writer_id = '110', days = '45' } = req.query;

    console.log(`ğŸ§ª Testing new BigQuery function for writer ${writer_id}, last ${days} days`);

    const endDate = new Date();
    const startDate = new Date();
    startDate.setDate(endDate.getDate() - parseInt(days));

    const startDateStr = startDate.toISOString().split('T')[0];
    const endDateStr = endDate.toISOString().split('T')[0];

    console.log(`ğŸ§ª Date range: ${startDateStr} to ${endDateStr}`);

    const result = await getBigQueryViews(writer_id, startDateStr, endDateStr, influxService);

    res.json({
      success: true,
      writerId: writer_id,
      dateRange: { start: startDateStr, end: endDateStr },
      dataPoints: result.length,
      data: result,
      message: 'New BigQuery function test completed'
    });

  } catch (error) {
    console.error('ğŸ§ª Test endpoint error:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      message: 'New BigQuery function test failed'
    });
  }
});

// Helper function to get daily InfluxDB data for main chart
async function getDailyInfluxData(writerId, days = 30) {
  try {
    const InfluxService = require('../services/influxService');
    const influx = new InfluxService();

    // Query to get daily view increments (not cumulative totals) from InfluxDB
    const flux = `
      from(bucket: "youtube_api")
          |> range(start: -${parseInt(days)}d)
          |> filter(fn: (r) =>
               r._measurement == "views" and
               r._field       == "views" and
               r.writer_id    == "${writerId}"
             )
          |> map(fn: (r) => ({ r with _value: int(v: r._value) }))
          |> group(columns: ["video_id"])
          |> sort(columns: ["_time"])
          |> difference(nonNegative: true, columns: ["_value"])
          |> group()
          |> aggregateWindow(every: 1d, fn: sum, createEmpty: false)
          |> keep(columns: ["_time", "_value"])
          |> sort(columns: ["_time"])
    `;

    console.log(`ğŸ“Š InfluxDB daily query for writer_id ${writerId}: ${flux}`);

    let influxResults = [];
    await new Promise((resolve, reject) => {
      influx.queryApi.queryRows(flux, {
        next(row, tableMeta) {
          const o = tableMeta.toObject(row);
          influxResults.push({
            _time: o._time,
            _value: o._value || 0
          });
        },
        error(error) {
          console.error(`ğŸ“Š InfluxDB daily queryRows error:`, error.message);
          reject(error);
        },
        complete() {
          console.log(`ğŸ“Š InfluxDB daily queryRows returned ${influxResults.length} raw results`);
          resolve();
        }
      });
    });

    // Convert to format expected by frontend with UTC to EST conversion
    const dailyData = influxResults.map(item => {
      const utcDate = new Date(item._time);
      // Convert UTC to EST (UTC-5, or UTC-4 for EDT)
      const estDate = new Date(utcDate.toLocaleString("en-US", {timeZone: "America/New_York"}));

      return {
        time: estDate.toISOString().split('T')[0], // YYYY-MM-DD format
        views: parseInt(item._value || 0),
        source: 'InfluxDB_Daily_Aggregation'
      };
    });

    console.log(`ğŸ“Š InfluxDB daily returned ${dailyData.length} daily data points`);
    if (dailyData.length > 0) {
      console.log(`ğŸ“Š Sample daily data:`, dailyData.slice(0, 3));
    }

    return dailyData;
  } catch (error) {
    console.error('ğŸ“Š InfluxDB daily data error:', error.message);
    return [];
  }
}

// Realtime analytics endpoint - Last 24 hours hourly data
router.get('/realtime', authenticateToken, async (req, res) => {
  try {
    const { hours = 24 } = req.query;
    const userId = req.user.id;

    console.log(`âš¡ Realtime analytics requested for user ${userId}, last ${hours} hours`);

    // Get writer info directly using the user ID from JWT token
    const writerResult = await pool.query(
      'SELECT id, name FROM writer WHERE login_id = $1',
      [userId]
    );

    if (writerResult.rows.length === 0) {
      return res.status(404).json({ error: 'Writer not found' });
    }

    const writer = writerResult.rows[0];
    const writerId = writer.id.toString(); // Convert to string for InfluxDB query

    console.log(`âš¡ Found writer: ${writer.name} (ID: ${writerId})`);

    // Calculate time range for last 24 hours
    const endTime = new Date();
    const startTime = new Date(endTime.getTime() - (parseInt(hours) * 60 * 60 * 1000));

    console.log(`âš¡ Time range: ${startTime.toISOString()} to ${endTime.toISOString()}`);

    // Query InfluxDB youtube_api bucket for hourly data
    let hourlyData = [];
    let totalViews = 0;

    try {
      const InfluxService = require('../services/influxService');
      const influx = new InfluxService();



      // Query to get hourly view increments (not cumulative totals) from InfluxDB
const flux = `
    from(bucket: "youtube_api")
        |> range(start: -${parseInt(hours) + 1}h, stop: -1h)
        |> filter(fn: (r) =>
             r._measurement == "views" and
             r._field       == "views" and
             r.writer_id    == "${writerId}"
           )
        |> map(fn: (r) => ({ r with _value: int(v: r._value) }))
        |> group(columns: ["video_id"])
        |> sort(columns: ["_time"])
        |> difference(nonNegative: true, columns: ["_value"])
        |> group()
        |> aggregateWindow(every: 1h, fn: sum, createEmpty: false)
        |> keep(columns: ["_time", "_value"])
        |> sort(columns: ["_time"])
        |> limit(n: ${parseInt(hours)})
`;


      console.log(`âš¡ InfluxDB youtube_api query for writer_id ${writerId}: ${flux}`);

      let influxResults = [];
      try {
        // Use Promise to ensure we wait for the query to complete
        await new Promise((resolve, reject) => {
          influx.queryApi.queryRows(flux, {
            next(row, tableMeta) {
              const o = tableMeta.toObject(row);
              influxResults.push({
                _time: o._time,
                _value: o._value || 0
              });
            },
            error(error) {
              console.error(`âš¡ InfluxDB queryRows error:`, error.message);
              reject(error);
            },
            complete() {
              console.log(`âš¡ InfluxDB queryRows returned ${influxResults.length} raw results`);
              resolve();
            }
          });
        });
      } catch (err) {
        console.error(`âš¡ InfluxDB query error:`, err.message);
        influxResults = [];
      }

      // Convert to format expected by frontend
      hourlyData = influxResults.map(item => ({
        time: new Date(item._time).toISOString(),
        views: parseInt(item._value || 0)
      }));

      totalViews = hourlyData.reduce((sum, item) => sum + item.views, 0);

      console.log(`âš¡ InfluxDB youtube_api returned ${hourlyData.length} hourly data points`);
      console.log(`âš¡ InfluxDB total views: ${totalViews.toLocaleString()}`);

      // Log sample data for debugging
      if (hourlyData.length > 0) {
        console.log(`âš¡ Sample hourly data:`, hourlyData.slice(0, 3));
      }

    } catch (influxError) {
      console.error('âš¡ InfluxDB error:', influxError.message);

      // Return empty data if InfluxDB fails
      hourlyData = [];
      totalViews = 0;
    }

    // Create a complete time series for the last N hours (fill missing hours with 0)
    const now = new Date();
    const completeHourlyData = [];

    // Generate all hours from N hours ago to now
    for (let i = parseInt(hours) - 1; i >= 0; i--) {
      const hourTime = new Date(now.getTime() - (i * 60 * 60 * 1000));

      // Find matching data point from InfluxDB
      const matchingData = hourlyData.find(item => {
        const itemTime = new Date(item.time);
        return Math.abs(itemTime.getTime() - hourTime.getTime()) < 30 * 60 * 1000; // Within 30 minutes
      });

      completeHourlyData.push({
        time: hourTime.toISOString(),
        views: matchingData ? matchingData.views : 0
      });
    }

    // Format chart data for the widget (complete N bars representing hourly data)
    const chartData = completeHourlyData.map((item) => {
      const date = new Date(item.time);
      // Convert UTC to EST for display
      const estDate = new Date(date.getTime() - (5 * 60 * 60 * 1000));

      return {
        time: estDate.toLocaleTimeString('en-US', {
          hour: '2-digit',
          minute: '2-digit',
          hour12: false
        }),
        views: item.views,
        hour: estDate.getHours()
      };
    });

    const response = {
      totalViews: totalViews,
      chartData: chartData,
      lastUpdated: new Date().toISOString(),
      timeRange: `${hours} hours`,
      dataPoints: chartData.length, // Use chartData length (complete series)
      influxDataPoints: hourlyData.length, // Original InfluxDB data points
      writerId: writerId,
      writerName: writer.name
    };

    console.log(`âš¡ Realtime response: ${totalViews.toLocaleString()} views, ${chartData.length} chart points (${hourlyData.length} from InfluxDB)`);

    res.json(response);

  } catch (error) {
    console.error('âš¡ Realtime endpoint error:', error);
    res.status(500).json({ error: 'Failed to fetch realtime data' });
  }
});

// Test endpoint for InfluxDB only
router.get('/test-influx-only', async (req, res) => {
  try {
    const { writer_id = '110', days = '7' } = req.query;

    console.log(`ğŸ§ª Testing InfluxDB only for writer ${writer_id}, last ${days} days`);

    const InfluxService = require('../services/influxService');
    const influxService = new InfluxService();

    const timeRange = `${days}d`;
    const influxData = await influxService.getDashboardAnalytics(timeRange, writer_id);

    console.log(`ğŸ“Š InfluxDB returned ${influxData.length} rows`);

    const formattedData = influxData.map(row => ({
      date: new Date(row.date).toISOString().split('T')[0],
      views: parseInt(row.views || 0)
    }));

    res.json({
      success: true,
      writerId: writer_id,
      timeRange: timeRange,
      dataPoints: formattedData.length,
      data: formattedData,
      message: 'InfluxDB test completed'
    });

  } catch (error) {
    console.error('ğŸ§ª InfluxDB test error:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      message: 'InfluxDB test failed'
    });
  }
});

// Test endpoint to check account name function
router.get('/test-account-name', async (req, res) => {
  try {
    const { video_id = '1', writer_id = '110' } = req.query;

    console.log(`ğŸ§ª Testing account name function for video ${video_id}, writer ${writer_id}`);

    const accountName = await getAccountNameFromBigQuery(video_id, writer_id);

    res.json({
      success: true,
      video_id: video_id,
      writer_id: writer_id,
      account_name: accountName,
      message: accountName ? 'Account name found' : 'No account name found'
    });

  } catch (error) {
    console.error('ğŸ§ª Account name test error:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      message: 'Account name test failed'
    });
  }
});

// Test endpoint to show exact top content API response structure
router.get('/test-top-content-response', async (req, res) => {
  try {
    const { writer_id = '110' } = req.query;

    console.log(`ğŸ§ª Testing top content response structure for writer ${writer_id}`);

    // Call the actual top content function to see what it returns
    const topContentQuery = `
      SELECT
        video.id as video_id,
        video.script_title AS title,
        video.url,
        COALESCE(statistics_youtube_api.views_total, 0) AS views,
        COALESCE(statistics_youtube_api.likes_total, 0) AS likes,
        COALESCE(statistics_youtube_api.comments_total, 0) AS comments,
        statistics_youtube_api.preview,
        statistics_youtube_api.duration,
        statistics_youtube_api.posted_date as first_date
      FROM video
      LEFT JOIN statistics_youtube_api
          ON CAST(video.id AS VARCHAR) = statistics_youtube_api.video_id
      WHERE video.writer_id = $1
        AND video.url LIKE '%youtube.com%'
        AND statistics_youtube_api.views_total IS NOT NULL
      ORDER BY statistics_youtube_api.views_total DESC
      LIMIT 3
    `;

    const result = await pool.query(topContentQuery, [parseInt(writer_id)]);
    const topContent = result.rows;

    // Try to enhance with account names
    const enhancedContent = await Promise.all(topContent.map(async (video) => {
      const accountName = await getAccountNameFromBigQuery(video.video_id, writer_id);

      return {
        id: video.video_id,
        title: video.title,
        url: video.url,
        views: video.views,
        likes: video.likes,
        comments: video.comments,
        account_name: accountName || 'Not Available',
        writer_name: 'Test Writer',
        thumbnail: video.preview,
        duration: video.duration
      };
    }));

    console.log('ğŸ§ª Test response structure:', {
      total_videos: enhancedContent.length,
      sample_video: enhancedContent[0],
      all_account_names: enhancedContent.map(v => v.account_name)
    });

    res.json({
      success: true,
      data: enhancedContent,
      metadata: {
        writer_id: writer_id,
        total_found: enhancedContent.length,
        source: 'Test Endpoint'
      },
      debug: {
        message: 'This shows the exact structure being sent to frontend',
        account_names: enhancedContent.map(v => ({
          title: v.title,
          account_name: v.account_name
        }))
      }
    });

  } catch (error) {
    console.error('ğŸ§ª Test top content response error:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      message: 'Test top content response failed'
    });
  }
});

// Test endpoint to verify top content functionality
router.get('/test/top-content', async (req, res) => {
  try {
    const { writer_id = '110', type = 'all' } = req.query;

    console.log('ğŸ§ª Testing top content endpoint with writer_id:', writer_id, 'type:', type);

    // Test the main endpoint
    const testUrl = `/api/analytics/writer/top-content?writer_id=${writer_id}&range=30&limit=20&type=${type}`;
    console.log('ğŸ§ª Test URL would be:', testUrl);

    res.json({
      message: 'Top content test endpoint',
      test_url: testUrl,
      expected_behavior: {
        all: '10 shorts + 10 videos (balanced)',
        shorts: 'Only videos < 183 seconds',
        content: 'Only videos >= 183 seconds'
      },
      bigquery_table: 'youtube_video_report_historical',
      account_name_source: 'channel_title'
    });

  } catch (error) {
    console.error('âŒ Test endpoint error:', error);
    res.status(500).json({ error: 'Test failed', details: error.message });
  }
});

module.exports = router;
module.exports.bigquery = bigquery;
